hadoop.http.filter.initializers	org.apache.hadoop.http.lib.StaticUserWebFilter	A comma separated list of class names. Each class in the list   must extend org.apache.hadoop.http.FilterInitializer. The corresponding   Filter will be initialized. Then, the Filter will be applied to all user   facing jsp and servlet web pages.  The ordering of the list defines the   ordering of the filters.
hadoop.security.group.mapping	org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback	Class for user to group mapping (get groups for a given user) for ACL.     The default implementation,     org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback,     will determine if the Java Native Interface (JNI) is available. If JNI is     available the implementation will use the API within hadoop to resolve a     list of groups for a user. If JNI is not available then the shell     implementation, ShellBasedUnixGroupsMapping, is used.  This implementation     shells out to the Linux/Unix environment with the     bash -c groups command to resolve a list of groups for a user.
hadoop.security.dns.log-slow-lookups.threshold.ms	1000	If slow lookup logging is enabled, this threshold is used to decide if a     lookup is considered slow enough to be logged.
hadoop.security.groups.cache.secs	300	This is the config controlling the validity of the entries in the cache     containing the user->group mapping. When this duration has expired,     then the implementation of the group mapping provider is invoked to get     the groups of the user and then cached back.
hadoop.security.groups.cache.warn.after.ms	5000	If looking up a single user to group takes longer than this amount of     milliseconds, we will log a warning message.
hadoop.security.groups.cache.background.reload.threads	3	Only relevant if hadoop.security.groups.cache.background.reload is true.     Controls the number of concurrent background user->group cache entry     refreshes. Pending refresh requests beyond this value are queued and     processed when a thread is free.
hadoop.security.group.mapping.ldap.connection.timeout.ms	60000	This property is the connection timeout (in milliseconds) for LDAP     operations. If the LDAP provider doesn't establish a connection within the     specified period, it will abort the connect attempt. Non-positive value     means no LDAP connection timeout is specified in which case it waits for the     connection to establish until the underlying network times out.
hadoop.security.group.mapping.ldap.search.filter.user	(&(objectClass=user)(sAMAccountName={0}))	An additional filter to use when searching for LDAP users. The default will     usually be appropriate for Active Directory installations. If connecting to     an LDAP server with a non-AD schema, this should be replaced with     (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to     denote where the username fits into the filter.      If the LDAP server supports posixGroups, Hadoop can enable the feature by     setting the value of this property to "posixAccount" and the value of     the hadoop.security.group.mapping.ldap.search.filter.group property to     "posixGroup".
hadoop.security.group.mapping.ldap.search.filter.group	(objectClass=group)	An additional filter to use when searching for LDAP groups. This should be     changed when resolving groups against a non-Active Directory installation.      See the description of hadoop.security.group.mapping.ldap.search.filter.user     to enable posixGroups support.
hadoop.security.group.mapping.ldap.search.attr.member	member	The attribute of the group object that identifies the users that are     members of the group. The default will usually be appropriate for     any LDAP installation.
hadoop.security.group.mapping.ldap.search.attr.group.name	cn	The attribute of the group object that identifies the group name. The     default will usually be appropriate for all LDAP systems.
hadoop.security.group.mapping.ldap.posix.attr.gid.name	gidNumber	The attribute of posixAccount indicating the group id.
hadoop.security.group.mapping.ldap.directory.search.timeout	10000	The attribute applied to the LDAP SearchControl properties to set a     maximum time limit when searching and awaiting a result.     Set to 0 if infinite wait period is desired.     Default is 10 seconds. Units in milliseconds.
hadoop.security.uid.cache.secs	14400	This is the config controlling the validity of the entries in the cache         containing the userId to userName and groupId to groupName used by         NativeIO getFstat().
hadoop.rpc.protection	authentication	A comma-separated list of protection values for secured sasl       connections. Possible values are authentication, integrity and privacy.       authentication means authentication only and no integrity or privacy;       integrity implies authentication and integrity are enabled; and privacy       implies all of authentication, integrity and privacy are enabled.       hadoop.security.saslproperties.resolver.class can be used to override       the hadoop.rpc.protection for a connection at the server side.
hadoop.security.saslproperties.resolver.class		SaslPropertiesResolver used to resolve the QOP used for a       connection. If not specified, the full set of values specified in       hadoop.rpc.protection is used while determining the QOP used for the       connection. If a class is specified, then the QOP values returned by       the class will be used while determining the QOP used for the connection.
hadoop.security.sensitive-config-keys	secret$       password$       ssl.keystore.pass$       fs.s3.*[Ss]ecret.?[Kk]ey       fs.azure.account.key.*       credential$       oauth.*token$       hadoop.security.sensitive-config-keys	A comma-separated or multi-line list of regular expressions to       match configuration keys that should be redacted where appropriate, for       example, when logging modified properties during a reconfiguration,       private credentials should not be logged.
hadoop.kerberos.kinit.command	kinit	Used to periodically renew Kerberos credentials when provided   to Hadoop. The default setting assumes that kinit is in the PATH of users   running the Hadoop client. Change this to the absolute path to kinit if this   is not the case.
hadoop.kerberos.min.seconds.before.relogin	60	The minimum time between relogin attempts for Kerberos, in     seconds.
io.file.buffer.size	4096	The size of buffer for use in sequence files.   The size of this buffer should probably be a multiple of hardware   page size (4096 on Intel x86), and it determines how much data is   buffered during read and write operations.
io.bytes.per.checksum	512	The number of bytes per checksum.  Must not be larger than   io.file.buffer.size.
io.serializations	org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization	A list of serialization classes that can be used for   obtaining serializers and deserializers.
io.seqfile.local.dir	${hadoop.tmp.dir}/io/local	The local directory where sequence file stores intermediate   data files during merge.  May be a comma-separated list of   directories on different devices in order to spread disk i/o.   Directories that do not exist are ignored.
io.map.index.interval	128	MapFile consist of two files - data file (tuples) and index file     (keys). For every io.map.index.interval records written in the     data file, an entry (record-key, data-file-position) is written     in the index file. This is to allow for doing binary search later     within the index file to look up records by their keys and get their     closest positions in the data file.
fs.defaultFS	file:///	The name of the default file system.  A URI whose   scheme and authority determine the FileSystem implementation.  The   uri's scheme determines the config property (fs.SCHEME.impl) naming   the FileSystem implementation class.  The uri's authority is used to   determine the host, port, etc. for a filesystem.
fs.trash.interval	0	Number of minutes after which the checkpoint   gets deleted.  If zero, the trash feature is disabled.   This option may be configured both on the server and the   client. If trash is disabled server side then the client   side configuration is checked. If trash is enabled on the   server side then the value configured on the server is   used and the client configuration value is ignored.
fs.trash.checkpoint.interval	0	Number of minutes between trash checkpoints.   Should be smaller or equal to fs.trash.interval. If zero,   the value is set to the value of fs.trash.interval.   Every time the checkpointer runs it creates a new checkpoint   out of current and removes checkpoints created more than   fs.trash.interval minutes ago.
fs.AbstractFileSystem.file.impl	org.apache.hadoop.fs.local.LocalFs	The AbstractFileSystem for file: uris.
fs.AbstractFileSystem.har.impl	org.apache.hadoop.fs.HarFs	The AbstractFileSystem for har: uris.
fs.AbstractFileSystem.viewfs.impl	org.apache.hadoop.fs.viewfs.ViewFs	The AbstractFileSystem for view file system for viewfs: uris   (ie client side mount table:).
fs.viewfs.rename.strategy	SAME_MOUNTPOINT	Allowed rename strategy to rename between multiple mountpoints.     Allowed values are SAME_MOUNTPOINT,SAME_TARGET_URI_ACROSS_MOUNTPOINT and     SAME_FILESYSTEM_ACROSS_MOUNTPOINT.
fs.ftp.host	0.0.0.0	FTP filesystem connects to this server
fs.df.interval	60000	Disk usage statistics refresh interval in msec.
fs.s3a.impl	org.apache.hadoop.fs.s3a.S3AFileSystem	The implementation class of the S3A Filesystem
io.seqfile.compress.blocksize	1000000	The minimum block size for compression in block compressed           SequenceFiles.
io.mapfile.bloom.error.rate	0.005	The rate of false positives in BloomFilter-s used in BloomMapFile.   As this value decreases, the size of BloomFilter-s increases exponentially. This   value is the probability of encountering false positives (default is 0.5%).
hadoop.util.hash.type	murmur	The default implementation of Hash. Currently this can take one of the   two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.
ipc.client.idlethreshold	4000	Defines the threshold number of connections after which                connections will be inspected for idleness.
ipc.client.kill.max	10	Defines the maximum number of clients to disconnect in one go.
ipc.client.connection.maxidletime	10000	The maximum time in msec after which a client will bring down the                connection to the server.
ipc.client.connect.max.retries	10	Indicates the number of retries a client will make to establish                a server connection.
ipc.client.connect.retry.interval	1000	Indicates the number of milliseconds a client will wait for     before retrying to establish a server connection.
ipc.ping.interval	60000	Timeout on waiting response from server, in milliseconds.   The client will send ping when the interval is passed without receiving bytes,   if ipc.client.ping is set to true.
ipc.client.rpc-timeout.ms	0	Timeout on waiting response from server, in milliseconds.   If ipc.client.ping is set to true and this rpc-timeout is greater than   the value of ipc.ping.interval, the effective value of the rpc-timeout is   rounded up to multiple of ipc.ping.interval.
ipc.maximum.data.length	67108864	This indicates the maximum IPC message length (bytes) that can be     accepted by the server. Messages larger than this value are rejected by the     immediately to avoid possible OOMs. This setting should rarely need to be     changed.
ipc.maximum.response.length	134217728	This indicates the maximum IPC message length (bytes) that can be     accepted by the client. Messages larger than this value are rejected     immediately to avoid possible OOMs. This setting should rarely need to be     changed.  Set to 0 to disable.
hadoop.rpc.socket.factory.class.default	org.apache.hadoop.net.StandardSocketFactory	Default SocketFactory to use. This parameter is expected to be     formatted as "package.FactoryClassName".
net.topology.script.file.name		The script name that should be invoked to resolve DNS names to     NetworkTopology names. Example: the script would take host.foo.bar as an     argument, and return /rack1 as the output.
net.topology.script.number.args	100	The max number of args that the script configured with     net.topology.script.file.name should be run with. Each arg is an     IP address.
net.topology.table.file.name		The file name for a topology file, which is used when the     net.topology.node.switch.mapping.impl property is set to     org.apache.hadoop.net.TableMapping. The file format is a two column text     file, with columns separated by whitespace. The first column is a DNS or     IP address and the second column specifies the rack where the address maps.     If no entry corresponding to a host in the cluster is found, then     /default-rack is assumed.
file.stream-buffer-size	4096	The size of buffer to stream files.   The size of this buffer should probably be a multiple of hardware   page size (4096 on Intel x86), and it determines how much data is   buffered during read and write operations.
file.bytes-per-checksum	512	The number of bytes per checksum.  Must not be larger than   file.stream-buffer-size
file.blocksize	67108864	Block size
file.replication	1	Replication factor
ftp.bytes-per-checksum	512	The number of bytes per checksum.  Must not be larger than   ftp.stream-buffer-size
ftp.client-write-packet-size	65536	Packet size for clients to write
ftp.replication	3	Replication factor
tfile.fs.output.buffer.size	262144	Buffer size used for FSDataOutputStream in bytes.
tfile.fs.input.buffer.size	262144	Buffer size used for FSDataInputStream in bytes.
hadoop.http.authentication.type	simple	Defines authentication used for Oozie HTTP endpoint.     Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
hadoop.http.authentication.signature.secret.file	${user.home}/hadoop-http-auth-signature-secret	The signature secret for signing the authentication tokens.     The same secret should be used for JT/NN/DN/TT configurations.
hadoop.http.authentication.kerberos.principal	HTTP/_HOST@LOCALHOST	Indicates the Kerberos principal to be used for HTTP endpoint.     The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
hadoop.http.authentication.kerberos.keytab	${user.home}/hadoop.keytab	Location of the keytab file with the credentials for the principal.     Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
hadoop.http.staticuser.user	dr.who	The user name to filter as, on static web filters     while rendering content. An example use is the HDFS     web UI (user to be used for browsing files).
ha.zookeeper.session-timeout.ms	10000	The session timeout to use when the ZKFC connects to ZooKeeper.     Setting this value to a lower value implies that server crashes     will be detected more quickly, but risks triggering failover too     aggressively in the case of a transient error or network blip.
ha.zookeeper.parent-znode	/hadoop-ha	The ZooKeeper znode under which the ZK failover controller stores     its information. Note that the nameservice ID is automatically     appended to this znode, so it is not normally necessary to     configure this, even in a federated environment.
ha.zookeeper.acl	world:anyone:rwcda	A comma-separated list of ZooKeeper ACLs to apply to the znodes     used by automatic failover. These ACLs are specified in the same     format as used by the ZooKeeper CLI.      If the ACL itself contains secrets, you may instead specify a     path to a file, prefixed with the '@' symbol, and the value of     this configuration will be loaded from within.
hadoop.ssl.keystores.factory.class	org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory	The keystores factory to use for retrieving certificates.
hadoop.ssl.server.conf	ssl-server.xml	Resource file from which ssl server keystore information will be extracted.     This file is looked up in the classpath, typically it should be in Hadoop     conf/ directory.
hadoop.ssl.client.conf	ssl-client.xml	Resource file from which ssl client keystore information will be extracted     This file is looked up in the classpath, typically it should be in Hadoop     conf/ directory.
hadoop.ssl.enabled.protocols	TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2	The supported SSL protocols.
fs.permissions.umask-mode	022	The umask used when creating files and directories.     Can be in octal or in symbolic. Examples are:     "022" (octal for u=rwx,g=r-x,o=r-x in symbolic),     or "u=rwx,g=rwx,o=" (symbolic for 007 in octal).
ha.health-monitor.rpc-timeout.ms	45000	Timeout for the actual monitorHealth() calls.
ha.failover-controller.new-active.rpc-timeout.ms	60000	Timeout that the FC waits for the new active to become active
ha.failover-controller.graceful-fence.rpc-timeout.ms	5000	Timeout that the FC waits for the old active to go to standby
ha.failover-controller.graceful-fence.connection.retries	1	FC connection retries for graceful fencing
ha.failover-controller.cli-check.rpc-timeout.ms	20000	Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState
hadoop.user.group.static.mapping.overrides	dr.who=;	Static mapping of user to groups. This will override the groups if     available in the system for the specified user. In otherwords, groups     look-up will not happen for these users, instead groups mapped in this     configuration will be used.     Mapping should be in this format.     user1=group1,group2;user2=;user3=group2;     Default, "dr.who=;" will consider "dr.who" as user without groups.
hadoop.security.crypto.cipher.suite	AES/CTR/NoPadding	Cipher suite for crypto codec.
hadoop.security.crypto.jce.provider		The JCE provider name used in CryptoCodec.
hadoop.security.crypto.buffer.size	8192	The buffer size used by CryptoInputStream and CryptoOutputStream.
hadoop.security.java.secure.random.algorithm	SHA1PRNG	The java secure random algorithm.
hadoop.security.random.device.file.path	/dev/urandom	OS security random device file path.
hadoop.security.kms.client.authentication.retry-count	1	Number of time to retry connecting to KMS on authentication failure
hadoop.security.kms.client.encrypted.key.cache.size	500	Size of the EncryptedKeyVersion cache Queue for each key
hadoop.security.kms.client.encrypted.key.cache.expiry	43200000	Cache expiry time for a Key, after which the cache Queue for this     key will be dropped. Default = 12hrs
hadoop.security.kms.client.failover.sleep.base.millis	100	Expert only. The time to wait, in milliseconds, between failover     attempts increases exponentially as a function of the number of     attempts made so far, with a random factor of +/- 50%. This option     specifies the base value used in the failover calculation. The     first failover will retry immediately. The 2nd failover attempt     will delay at least hadoop.security.client.failover.sleep.base.millis     milliseconds. And so on.
fs.client.htrace.sampler.classes		The class names of the HTrace Samplers to use for Hadoop       filesystem clients.
hadoop.caller.context.max.size	128	The maximum bytes a caller context string can have. If the       passed caller context is longer than this maximum bytes, client will       truncate it before sending to server. Note that the server may have a       different maximum size, and will truncate the caller context to the       maximum size it allows.
hadoop.caller.context.signature.max.size	40	The caller's signature (optional) is for offline validation. If the       signature exceeds the maximum allowed bytes in server, the caller context       will be abandoned, in which case the caller context will not be recorded       in audit logs.
