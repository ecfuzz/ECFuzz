alluxio.job.master.embedded.journal.addresses		A comma-separated list of journal addresses for all job masters in the cluster. The format is 'hostname1:port1,hostname2:port2,...'. Defaults to the journal addresses set for the Alluxio masters (alluxio.master.embedded.journal.addresses), but with the job master embedded journal port.
alluxio.job.master.hostname	${alluxio.master.hostname}	The hostname of the Alluxio job master.
alluxio.job.master.rpc.addresses		The list of RPC addresses to use for the job service configured in non-zookeeper HA mode. If this property is not specifically defined, it will first fall back to using alluxio.master.rpc.addresses, replacing those address ports with the port defined by alluxio.job.master.rpc.port. Otherwise the addresses are inherited from alluxio.job.master.embedded.journal.addresses using the port defined in alluxio.job.master.rpc.port
alluxio.job.master.web.bind.host	0.0.0.0	The host that the job master web server binds to.
alluxio.job.master.web.port	20002	The port the job master web server uses.
alluxio.job.worker.web.port	30003	The port the Alluxio job worker web server uses.
alluxio.jvm.monitor.info.threshold	1sec	Extra sleep time longer than this threshold, log INFO.
alluxio.jvm.monitor.warn.threshold	10sec	Extra sleep time longer than this threshold, log WARN.
alluxio.locality.order	node,rack	Ordering of locality tiers
alluxio.master.audit.logging.enabled	false	Set to true to enable file system master audit.
alluxio.master.audit.logging.queue.capacity	10000	Capacity of the queue used by audit logging.
alluxio.master.backup.directory	/alluxio_backups	Default directory for writing master metadata backups. This path is an absolute path of the root UFS. For example, if the root ufs directory is hdfs://host:port/alluxio/data, the default backup directory will be hdfs://host:port/alluxio_backups.
alluxio.master.backup.entry.buffer.count	10000	How many journal entries to buffer during a back-up.
alluxio.master.embedded.journal.election.timeout	10s	The election timeout for the embedded journal. When this period elapses without a master receiving any messages, the master will attempt to become the primary. Election timeout will be waited initially when the cluster is forming. So larger values for election timeout will cause longer start-up time. Smaller values might introduce instability to leadership.
alluxio.master.embedded.journal.port	19200	The port to use for embedded journal communication with other masters.
alluxio.master.embedded.journal.shutdown.timeout	10sec	Maximum time to wait for embedded journal to stop on shutdown.
alluxio.master.embedded.journal.storage.level	DISK	The storage level for storing embedded journal logs. Use DISK for maximum durability. Use MAPPED for better performance, but some risk of losing state in case of power loss or host failure. Use MEMORY for optimal performance, but no state persistence across cluster restarts.
alluxio.master.journal.checkpoint.period.entries	2000000	The number of journal entries to write before creating a new journal checkpoint.
alluxio.master.journal.retry.interval	1sec	The amount of time to sleep between retrying journal flushes
alluxio.master.journal.tailer.shutdown.quiet.wait.time	5sec	Before the standby master shuts down its tailer thread, there should be no update to the leader master's journal in this specified time period.
alluxio.master.jvm.monitor.enabled	false	Whether to enable start JVM monitor thread on master.
alluxio.master.metastore	HEAP	The type of metastore to use, either HEAP or ROCKS. The heap metastore keeps all metadata on-heap, while the rocks metastore stores some metadata on heap and some metadata on disk. The rocks metastore has the advantage of being able to support a large namespace (1 billion plus files) without needing a massive heap size.
alluxio.master.metastore.inode.cache.high.water.mark.ratio	0.85	The high water mark for the inode cache, as a ratio from high water mark to total cache size. If this is 0.85 and the max size is 10 million, the high water mark value is 8.5 million. When the cache reaches the high water mark, the eviction process will evict down to the low water mark.
alluxio.master.metastore.inode.cache.max.size	10000000	The number of inodes to cache on-heap. This only applies to off-heap metastores, e.g. ROCKS. Set this to 0 to disable the on-heap inode cache
alluxio.master.metrics.time.series.interval	5min	Interval for which the master records metrics information. This affects the granularity of the metrics graphed in the UI.
alluxio.master.mount.table.root.alluxio	/	Alluxio root mount point.
alluxio.master.mount.table.root.readonly	false	Whether Alluxio root mount point is readonly.
alluxio.master.periodic.block.integrity.check.interval	1hr	The period for the block integrity check, disabled if <= 0.
alluxio.master.rpc.addresses		A list of comma-separated host:port RPC addresses where the client should look for masters when using multiple masters without Zookeeper. This property is not used when Zookeeper is enabled, since Zookeeper already stores the master addresses.
alluxio.master.rpc.executor.core.pool.size	0	the number of threads to keep in thread pool of master RPC executor service. By default it is same as the parallelism level, but may be set to a larger value to reduce dynamic overhead if tasks regularly block. A smaller value (for example 0) is equivalent to the default.
alluxio.master.rpc.executor.keepalive	60sec	the keep alive time of a thread in master RPC executor service last used before this thread is terminated (and replaced if necessary).
alluxio.master.rpc.executor.max.pool.size	500	the maximum number of threads allowed for master RPC executor service. When the maximum is reached, attempts to replace blocked threads fail.
alluxio.master.rpc.executor.min.runnable	1	the minimum allowed number of core threads not blocked. To ensure progress, when too few unblocked threads exist and unexecuted tasks may exist, new threads are constructed up to the value of alluxio.master.rpc.executor.max.pool.size. A value of 1 ensures liveness. A larger value might improve throughput but might also increase overhead.
alluxio.master.rpc.port	19998	The port for Alluxio master's RPC service.
alluxio.master.standby.heartbeat.interval	2min	The heartbeat interval between Alluxio primary master and standby masters.
alluxio.master.startup.block.integrity.check.enabled	true	Whether the system should be checked on startup for orphaned blocks (blocks having no corresponding files but still taking system resource due to various system failures). Orphaned blocks will be deleted during master startup if this property is true. This property is available since 1.7.1
alluxio.master.tieredstore.global.mediumtype	MEM, SSD, HDD	The list of medium types we support in the system.
alluxio.master.ufs.active.sync.interval	30sec	Time interval to periodically actively sync UFS
alluxio.master.web.bind.host	0.0.0.0	The hostname Alluxio master web UI binds to.
alluxio.master.whitelist	/	A comma-separated list of prefixes of the paths which are cacheable, separated by semi-colons. Alluxio will try to cache the cacheable file when it is read for the first time.
alluxio.master.worker.connect.wait.time	5sec	Alluxio master will wait a period of time after start up for all workers to register, before it starts accepting client requests. This property determines the wait time.
alluxio.master.worker.timeout	5min	Timeout between master and worker indicating a lost worker.
alluxio.network.connection.health.check.timeout	5sec	Allowed duration for checking health of client connections (gRPC channels) before being assigned to a client. If a connection does not become active within configured time, it will be shut down and a new connection will be created for the client
alluxio.network.connection.server.shutdown.timeout	60sec	Maximum time to wait for gRPC server to stop on shutdown
alluxio.network.host.resolution.timeout	5sec	During startup of the Master and Worker processes Alluxio needs to ensure that they are listening on externally resolvable and reachable host names. To do this, Alluxio will automatically attempt to select an appropriate host name if one was not explicitly specified. This represents the maximum amount of time spent waiting to determine if a candidate host name is resolvable over the network.
alluxio.proxy.stream.cache.timeout	1hour	The timeout for the input and output streams cache eviction in the proxy.
alluxio.proxy.web.bind.host	0.0.0.0	The hostname that the Alluxio proxy's web server runs on.
alluxio.security.authorization.permission.enabled	true	Whether to enable access control based on file permission.
alluxio.security.group.mapping.cache.timeout	1min	Time for cached group mapping to expire.
alluxio.security.group.mapping.class	alluxio.security.group.provider.ShellBasedUnixGroupsMapping	The class to provide user-to-groups mapping service. Master could get the various group memberships of a given user. It must implement the interface 'alluxio.security.group.GroupMappingService'. The default implementation execute the 'groups' shell command to fetch the group memberships of a given user.
alluxio.underfs.cleanup.enabled	false	Whether or not to clean up under file storage periodically.Some ufs operations may not be completed and cleaned up successfully in normal ways and leave some intermediate data that needs periodical cleanup.If enabled, all the mount points will be cleaned up when a leader master starts or cleanup interval is reached. This should be used sparingly.
alluxio.user.block.avoid.eviction.policy.reserved.size.bytes	0MB	The portion of space reserved in a worker when using the LocalFirstAvoidEvictionPolicy class as block location policy.
alluxio.user.block.worker.client.pool.size	1024	The maximum number of block worker clients cached in the block worker client pool.
alluxio.user.file.buffer.bytes	8MB	The size of the file buffer to use for file system reads/writes.
alluxio.user.file.create.ttl.action	DELETE	When file's ttl is expired, the action performs on it. Options: DELETE (default) or FREE
alluxio.user.file.master.client.threads	10	The number of threads used by a file master client to talk to the file master.
alluxio.user.file.metadata.sync.interval	-1	The interval for syncing UFS metadata before invoking an operation on a path. -1 means no sync will occur. 0 means Alluxio will always sync the metadata of the path before an operation. If you specify a time interval, Alluxio will (best effort) not re-sync a path within that time interval. Syncing the metadata for a path must interact with the UFS, so it is an expensive operation. If a sync is performed for an operation, the configuration of "alluxio.user.file.metadata.load.type" will be ignored.
alluxio.user.file.persistence.initial.wait.time	0	Time to wait before starting the persistence job. When the value is set to -1, the file will be persisted by rename operation or persist CLI but will not be automatically persisted in other cases. This is to avoid the heavy object copy in rename operation when alluxio.user.file.writetype.default is set to ASYNC_THROUGH. This value should be smaller than the value of alluxio.master.persistence.max.total.wait.time
alluxio.user.file.readtype.default	CACHE_PROMOTE	Default read type when creating Alluxio files. Valid options are `CACHE_PROMOTE` (move data to highest tier if already in Alluxio storage, write data into highest tier of local Alluxio if data needs to be read from under storage), `CACHE` (write data into highest tier of local Alluxio if data needs to be read from under storage), `NO_CACHE` (no data interaction with Alluxio, if the read is from Alluxio data migration or eviction will not occur).
alluxio.user.file.replication.min	0	The target min replication level of a file in Alluxio space.
alluxio.user.metrics.collection.enabled	false	Enable collecting the client-side metrics and heartbeat them to master
alluxio.user.metrics.heartbeat.interval	3sec	The time period of client master heartbeat to send the client-side metrics.
alluxio.user.network.netty.channel	EPOLL	Type of netty channels. If EPOLL is not available, this will automatically fall back to NIO.
alluxio.user.network.writer.buffer.size.messages	16	When a client writes to a remote worker, the maximum number of messages to buffer by the client. A message can be either a command response, a data chunk, or a gRPC stream event such as complete or error.
alluxio.user.rpc.retry.base.sleep	50ms	Alluxio client RPCs automatically retry for transient errors with an exponential backoff. This property determines the base time in the exponential backoff.
alluxio.user.rpc.retry.max.duration	2min	Alluxio client RPCs automatically retry for transient errors with an exponential backoff. This property determines the maximum duration to retry for before giving up. Note that, this value is set to 5s for fs and fsadmin CLIs.
alluxio.user.rpc.retry.max.sleep	3sec	Alluxio client RPCs automatically retry for transient errors with an exponential backoff. This property determines the maximum wait time in the backoff.
alluxio.user.short.circuit.enabled	true	The short circuit read/write which allows the clients to read/write data without going through Alluxio workers if the data is local is enabled if set to true.
alluxio.user.ufs.block.location.all.fallback.enabled	false	Whether to return all workers as block location if ufs block locations are not co-located with any Alluxio workers or is empty.
alluxio.user.ufs.block.read.location.policy	alluxio.client.block.policy.LocalFirstPolicy	When an Alluxio client reads a file from the UFS, it delegates the read to an Alluxio worker. The client uses this policy to choose which worker to read through. Built-in choices: [<a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/DeterministicHashPolicy.html">alluxio.client.block.policy.DeterministicHashPolicy</a>, <a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/LocalFirstAvoidEvictionPolicy.html">alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy</a>, <a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/LocalFirstPolicy.html">alluxio.client.block.policy.LocalFirstPolicy</a>, <a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/MostAvailableFirstPolicy.html">alluxio.client.block.policy.MostAvailableFirstPolicy</a>, <a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/RoundRobinPolicy.html">alluxio.client.block.policy.RoundRobinPolicy</a>, <a href="https://docs.alluxio.io/os/javadoc/edge/alluxio/client/block/policy/SpecificHostPolicy.html">alluxio.client.block.policy.SpecificHostPolicy</a>].
alluxio.user.ufs.block.read.location.policy.deterministic.hash.shards	1	When alluxio.user.ufs.block.read.location.policy is set to alluxio.client.block.policy.DeterministicHashPolicy, this specifies the number of hash shards.
alluxio.web.refresh.interval	15s	The amount of time to await before refreshing the Web UI if it is set to auto refresh.
alluxio.worker.allocator.class	alluxio.worker.block.allocator.MaxFreeAllocator	The strategy that a worker uses to allocate space among storage directories in certain storage layer. Valid options include: `alluxio.worker.block.allocator.MaxFreeAllocator`, `alluxio.worker.block.allocator.GreedyAllocator`, `alluxio.worker.block.allocator.RoundRobinAllocator`.
alluxio.worker.block.heartbeat.interval	1sec	The interval between block workers' heartbeats.
alluxio.worker.block.master.client.pool.size	11	The block master client pool size on the Alluxio workers.
alluxio.worker.data.folder.permissions	rwxrwxrwx	The permission set for the worker data folder. If short circuit is used this folder should be accessible by all users (rwxrwxrwx).
alluxio.worker.data.folder.tmp	.tmp_blocks	A relative path in alluxio.worker.data.folder used to store the temporary data for uncommitted files.
alluxio.worker.file.buffer.size	1MB	The buffer size for worker to write data into the tiered storage.
alluxio.worker.file.persist.pool.size	64	The size of the thread pool per worker, in which the thread persists an ASYNC_THROUGH file to under storage.
alluxio.worker.network.netty.boss.threads	1	How many threads to use for accepting new requests.
alluxio.worker.network.shutdown.timeout	15sec	Maximum amount of time to wait until the worker gRPC server is shutdown (regardless of the quiet period).
alluxio.worker.tieredstore.block.locks	1000	Total number of block locks for an Alluxio block worker. Larger value leads to finer locking granularity, but uses more space.
alluxio.worker.tieredstore.level0.dirs.path	/mnt/ramdisk on Linux, /Volumes/ramdisk on OSX	The path of storage directory for the top storage tier. Note for MacOS the value should be `/Volumes/`.
alluxio.worker.tieredstore.level0.watermark.high.ratio	0.95	The high watermark of the space in the top storage tier (a value between 0 and 1).
alluxio.worker.tieredstore.level1.watermark.low.ratio	0.7	The low watermark of the space in the second storage tier (a value between 0 and 1).
alluxio.worker.tieredstore.levels	1	The number of storage tiers on the worker.
alluxio.worker.web.hostname		The hostname Alluxio worker's web UI binds to.
alluxio.zookeeper.job.election.path	/job_election	
alluxio.zookeeper.job.leader.path	/job_leader	
alluxio.zookeeper.leader.path	/alluxio/leader	Leader directory in ZooKeeper.