Dependency Taxonomy,Configuration Parameter A,Configuration Parameter B,Class,Function
Control Dependency,hbase.master.port,hbase.regionserver.info.port,org.apache.hadoop.hbase.master.HMaster,<org.apache.hadoop.hbase.master.HMaster: int getRegionServerInfoPort(org.apache.hadoop.hbase.ServerName)>
Control Dependency,hbase.rest-csrf.browser-useragents-regex,hbase.rest.csrf.enabled,org.apache.hadoop.hbase.rest.RESTServer,*
Control Dependency,hbase.snapshot.restore.failsafe.name,hbase.snapshot.restore.take.failsafe.snapshot,org.apache.hadoop.hbase.client.HBaseAdmin,"<org.apache.hadoop.hbase.client.HBaseAdmin: void restoreSnapshot(java.lang.String,boolean,boolean)>"
Control Dependency,hbase.regionserver.info.bindAddress,hbase.regionserver.info.port.auto,org.apache.hadoop.hbase.regionserver.HRegionServer,<org.apache.hadoop.hbase.regionserver.HRegionServer: int putUpWebUI()>
Control Dependency,hbase.storescanner.parallel.seek.enable,hbase.storescanner.parallel.seek.threads,org.apache.hadoop.hbase.regionserver.HRegionServer,<org.apache.hadoop.hbase.regionserver.HRegionServer: void startServices()>
Control Dependency,hbase.defaults.for.version,hbase.defaults.for.version.skip,org.apache.hadoop.hbase.HBaseConfiguration,<org.apache.hadoop.hbase.HBaseConfiguration: void checkDefaultsVersion(org.apache.hadoop.conf.Configuration)>
Control Dependency,hbase.security.authentication,hbase.superuser,org.apache.hadoop.hbase.zookeeper.ZKUtil,"<org.apache.hadoop.hbase.zookeeper.ZKUtil: java.util.ArrayList createACL(org.apache.hadoop.hbase.zookeeper.ZKWatcher,java.lang.String,boolean)>"
Control Dependency,hbase.cluster.distributed,hbase.zookeeper.property.dataDir,org.apache.hadoop.hbase.master.HMasterCommandLine,<org.apache.hadoop.hbase.master.HMasterCommandLine: int startMaster()>
Control Dependency,hbase.coprocessor.enabled,hbase.coprocessor.user.enabled,org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost,*
Control Dependency,hbase.master.info.port.orig,hbase.master.infoserver.redirect,org.apache.hadoop.hbase.master.HMaster,*
Control Dependency,hbase.data.umask,hbase.data.umask.enable,org.apache.hadoop.hbase.regionserver.HRegionFileSystem,"<org.apache.hadoop.hbase.regionserver.HRegionFileSystem: boolean mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>"
Control Dependency,hbase.auth.token.max.lifetime,hbase.security.authentication,org.apache.hadoop.hbase.ipc.RpcServer,<org.apache.hadoop.hbase.ipc.RpcServer: org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager createSecretManager()>
Control Dependency,hbase.hstore.compaction.ratio.offpeak,hbase.offpeak.end.hour,org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours,*
Control Dependency,hbase.zookeeper.property.clientPort,hbase.zookeeper.quorum,org.apache.hadoop.hbase.master.HMasterCommandLine,<org.apache.hadoop.hbase.master.HMasterCommandLine: int startMaster()>
Control Dependency,dfs.internal.nameservices,dfs.namenode.checkpoint.dir,org.apache.hadoop.hbase.util.FSHDFSUtils,"<org.apache.hadoop.hbase.util.FSHDFSUtils: boolean isSameHdfs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)>"
Control Dependency,hbase.auth.key.update.interval,hbase.security.authentication,org.apache.hadoop.hbase.ipc.RpcServer,<org.apache.hadoop.hbase.ipc.RpcServer: org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager createSecretManager()>
Control Dependency,hbase.cluster.distributed,hbase.zookeeper.quorum,org.apache.hadoop.hbase.master.HMasterCommandLine,<org.apache.hadoop.hbase.master.HMasterCommandLine: int startMaster()>
Control Dependency,fs.defaultFS,io.file.buffer.size,org.apache.hadoop.hbase.util.CommonFSUtils,<org.apache.hadoop.hbase.util.CommonFSUtils: int getDefaultBufferSize(org.apache.hadoop.fs.FileSystem)>
Control Dependency,hbase.hstore.compaction.ratio.offpeak,hbase.offpeak.start.hour,org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours,*
Control Dependency,spark.streaming.backpressure.enabled,spark.streaming.backpressure.pid.derived,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.dynamicAllocation.enabled,spark.dynamicAllocation.schedulerBacklogTimeout,org.apache.spark.SparkContext,*
Control Dependency,spark.executor.uri,spark.mesos.executor.home,org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend,"<org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend: org.apache.mesos.Protos$CommandInfo createCommand(org.apache.mesos.Protos$Offer,int,java.lang.String)>"
Control Dependency,spark.cleaner.referenceTracking,spark.cleaner.referenceTracking.blocking,org.apache.spark.storage.BlockManager,*
Control Dependency,spark.task.reaper.enabled,spark.task.reaper.threadDump,org.apache.spark.executor.Executor$TaskReaper,*
Control Dependency,spark.history.kerberos.enabled,spark.history.kerberos.keytab,org.apache.spark.deploy.history.HistoryServer$,<org.apache.spark.deploy.history.HistoryServer$: void initSecurity()>
Control Dependency,CONTAINER_ID,spark.local.dir,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: java.lang.String getYarnLocalDirs(org.apache.spark.SparkConf)>
Control Dependency,spark.executor.extraLibraryPath,spark.executor.uri,org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,<org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler: java.lang.String getDriverCommandValue(org.apache.spark.deploy.mesos.MesosDriverDescription)>
Control Dependency,SPARK_HOME,SPARK_JARS,org.apache.spark.deploy.yarn.Client,"<org.apache.spark.deploy.yarn.Client: scala.collection.mutable.HashMap prepareLocalResources(org.apache.hadoop.fs.Path,scala.collection.Seq)>"
Control Dependency,spark.task.reaper.enabled,spark.task.reaper.killTimeout,org.apache.spark.executor.Executor$TaskReaper,*
Control Dependency,spark.streaming.backpressure.enabled,spark.streaming.backpressure.pid.proportional,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.memory.useLegacyMode,spark.storage.memoryFraction,org.apache.spark.memory.StaticMemoryManager,*
Control Dependency,spark.task.reaper.enabled,spark.task.reaper.pollingInterval,org.apache.spark.executor.Executor$TaskReaper,*
Control Dependency,spark.executor.logs.rolling.maxSize,spark.executor.logs.rolling.strategy,org.apache.spark.util.logging.FileAppender$,"<org.apache.spark.util.logging.FileAppender$: org.apache.spark.util.logging.FileAppender apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)>"
Control Dependency,spark.eventLog.compress,spark.eventLog.enabled,org.apache.spark.SparkContext,*
Control Dependency,spark.memory.offHeap.enabled,spark.memory.offHeap.size,org.apache.spark.memory.MemoryPool,<org.apache.spark.memory.MemoryPool: void incrementPoolSize(long)>
Control Dependency,spark.streaming.backpressure.enabled,spark.streaming.backpressure.rateEstimator,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.dynamicAllocation.enabled,spark.master,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: boolean isLocalMaster(org.apache.spark.SparkConf)>
Control Dependency,spark.dynamicAllocation.enabled,spark.dynamicAllocation.minExecutors,org.apache.spark.SparkContext,*
Control Dependency,spark.memory.useLegacyMode,spark.shuffle.memoryFraction,org.apache.spark.memory.StaticMemoryManager,*
Control Dependency,spark.history.kerberos.enabled,spark.history.kerberos.principal,org.apache.spark.deploy.history.HistoryServer$,<org.apache.spark.deploy.history.HistoryServer$: void initSecurity()>
Control Dependency,spark.eventLog.enabled,spark.eventLog.logBlockUpdates.enabled,org.apache.spark.SparkContext,*
Control Dependency,spark.deploy.recoveryMode,spark.deploy.zookeeper.url,org.apache.spark.deploy.master.Master,<org.apache.spark.deploy.master.Master: void onStart()>
Control Dependency,spark.executor.logs.rolling.time.interval,spark.executor.logs.rolling.strategy,org.apache.spark.util.logging.FileAppender$,"<org.apache.spark.util.logging.FileAppender$: org.apache.spark.util.logging.FileAppender apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)>"
Control Dependency,spark.memory.storageFraction,spark.memory.useLegacyMode,org.apache.spark.memory.UnifiedMemoryManager$,"<org.apache.spark.memory.UnifiedMemoryManager$: org.apache.spark.memory.UnifiedMemoryManager apply(org.apache.spark.SparkConf,int)>"
Control Dependency,spark.speculation,spark.speculation.interval,org.apache.spark.scheduler.TaskSchedulerImpl,<org.apache.spark.scheduler.TaskSchedulerImpl: void start()>
Control Dependency,spark.driver.cores,spark.submit.deployMode,org.apache.spark.SparkContext$,"<org.apache.spark.SparkContext$: int numDriverCores(java.lang.String,org.apache.spark.SparkConf)>"
Control Dependency,spark.streaming.backpressure.enabled,spark.streaming.backpressure.pid.minRate,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.streaming.backpressure.enabled,spark.streaming.backpressure.pid.integral,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.memory.useLegacyMode,spark.storage.unrollFraction,org.apache.spark.memory.StaticMemoryManager,*
Control Dependency,spark.dynamicAllocation.enabled,spark.dynamicAllocation.executorIdleTimeout,org.apache.spark.SparkContext,*
Control Dependency,spark.eventLog.dir,spark.eventLog.enabled,org.apache.spark.SparkContext,*
Control Dependency,MESOS_SANDBOX,spark.shuffle.service.enabled,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: java.lang.String[] getConfiguredLocalDirs(org.apache.spark.SparkConf)>
Control Dependency,spark.dynamicAllocation.testing,spark.master,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: boolean isDynamicAllocationEnabled(org.apache.spark.SparkConf)>
Control Dependency,spark.streaming.backpressure.pid.derived,spark.streaming.backpressure.rateEstimator,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,spark.dynamicAllocation.cachedExecutorIdleTimeout,spark.dynamicAllocation.enabled,org.apache.spark.SparkContext,*
Control Dependency,spark.cleaner.referenceTracking,spark.cleaner.referenceTracking.blocking.shuffle,org.apache.spark.storage.BlockManager,*
Control Dependency,CONTAINER_ID,SPARK_EXECUTOR_DIRS,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: java.lang.String[] getConfiguredLocalDirs(org.apache.spark.SparkConf)>
Control Dependency,spark.streaming.backpressure.pid.minRate,spark.streaming.backpressure.rateEstimator,org.apache.spark.streaming.scheduler.rate.RateEstimator$,"<org.apache.spark.streaming.scheduler.rate.RateEstimator$: org.apache.spark.streaming.scheduler.rate.RateEstimator create(org.apache.spark.SparkConf,org.apache.spark.streaming.Duration)>"
Control Dependency,hadoop.security.group.mapping.ldap.ssl,hadoop.security.group.mapping.ldap.ssl.truststore,*,*
Control Dependency,hadoop.security.group.mapping.ldap.ssl,hadoop.security.group.mapping.ldap.ssl.keystore.password,*,*
Control Dependency,hadoop.security.group.mapping.ldap.ssl,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,*,*
Control Dependency,alluxio.master.hostname,alluxio.user.network.netty.channel.pool.gc.threshold,alluxio.client.file.FileSystemContext,<alluxio.client.file.FileSystemContext: io.netty.channel.Channel acquireNettyChannel(alluxio.wire.WorkerNetAddress)>
Control Dependency,alluxio.integration.mesos.jdk.path,alluxio.integration.mesos.jdk.url,alluxio.mesos.AlluxioScheduler,<alluxio.mesos.AlluxioScheduler: java.lang.String createStartAlluxioCommand(java.lang.String)>
Control Dependency,alluxio.worker.file.persist.rate.limit,alluxio.worker.file.persist.rate.limit.enabled,alluxio.client.block.stream.PacketWriter$Factory,*
Control Dependency,alluxio.security.authentication.type,alluxio.security.login.impersonation.username,alluxio.security.authentication.AuthenticationProvider,*
Control Dependency,alluxio.user.short.circuit.enabled,alluxio.worker.data.server.domain.socket.as.uuid,alluxio.util.network.NettyUtils,<alluxio.util.network.NettyUtils: boolean isDomainSocketSupported(alluxio.wire.WorkerNetAddress)>
Control Dependency,lluxio.master.mount.table.root.option,fs.swift.simulation,alluxio.underfs.swift.SwiftUnderFileSystemFactory,<alluxio.underfs.swift.SwiftUnderFileSystemFactory: boolean checkSwiftCredentials(alluxio.underfs.UnderFileSystemConfiguration)>
Control Dependency,alluxio.user.hostname,alluxio.user.local.writer.packet.size.bytes,alluxio.client.block.stream.LocalFilePacketWriter,"<alluxio.client.block.stream.LocalFilePacketWriter: alluxio.client.block.stream.LocalFilePacketWriter create(alluxio.client.file.FileSystemContext,alluxio.wire.WorkerNetAddress,long,alluxio.client.file.options.OutStreamOptions)>"
Control Dependency,alluxio.underfs.s3a.inherit_acl,alluxio.underfs.s3.owner.id.to.username.mapping,alluxio.underfs.s3a.S3AUnderFileSystem,<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.ObjectUnderFileSystem$ObjectPermissions getPermissionsInternal()>
Control Dependency,alluxio.master.mount.table.root.option,alluxio.worker.principal,alluxio.underfs.hdfs.HdfsUnderFileSystem,<alluxio.underfs.hdfs.HdfsUnderFileSystem: void connectFromWorker(java.lang.String)>
Control Dependency,alluxio.master.mount.table.root.option,alluxio.master.principal,alluxio.underfs.hdfs.HdfsUnderFileSystem,<alluxio.underfs.hdfs.HdfsUnderFileSystem: void connectFromMaster(java.lang.String)>
Control Dependency,alluxio.master.hostname,alluxio.worker.data.server.domain.socket.as.uuid,alluxio.util.network.NetworkAddressUtils,<alluxio.util.network.NetworkAddressUtils: java.lang.String getClientHostName()>
Control Dependency,alluxio.master.mount.table.root.option,aws.accessKeyId,alluxio.underfs.s3a.S3AUnderFileSystem,<alluxio.underfs.s3a.S3AUnderFileSystem: com.amazonaws.auth.AWSCredentialsProvider createAwsCredentialsProvider(alluxio.underfs.UnderFileSystemConfiguration)>
Control Dependency,alluxio.underfs.listing.length,alluxio.underfs.s3a.list.objects.v1,alluxio.underfs.ObjectUnderFileSystem,<alluxio.underfs.ObjectUnderFileSystem: int getListingChunkLength()>
Control Dependency,alluxio.home,alluxio.integration.mesos.alluxio.jar.url,alluxio.mesos.AlluxioScheduler,<alluxio.mesos.AlluxioScheduler: java.lang.String createStartAlluxioCommand(java.lang.String)>
Control Dependency,alluxio.master.hostname,alluxio.user.local.reader.packet.size.bytes,alluxio.client.block.stream.BlockInStream,"<alluxio.client.block.stream.BlockInStream: alluxio.client.block.stream.BlockInStream createLocalBlockInStream(alluxio.client.file.FileSystemContext,alluxio.wire.WorkerNetAddress,long,long,alluxio.client.file.options.InStreamOptions)>"
Control Dependency,alluxio.user.hostname,alluxio.user.local.reader.packet.size.bytes,alluxio.client.block.stream.BlockInStream,"<alluxio.client.block.stream.BlockInStream: alluxio.client.block.stream.BlockInStream createLocalBlockInStream(alluxio.client.file.FileSystemContext,alluxio.wire.WorkerNetAddress,long,long,alluxio.client.file.options.InStreamOptions)>"
Control Dependency,alluxio.master.mount.table.root.option,alluxio.underfs.s3.proxy.port,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Control Dependency,alluxio.master.hostname,alluxio.user.network.netty.channel.pool.size.max,alluxio.client.file.FileSystemContext,<alluxio.client.file.FileSystemContext: io.netty.channel.Channel acquireNettyChannel(alluxio.wire.WorkerNetAddress)>
Control Dependency,alluxio.locality.order,alluxio.user.hostname,alluxio.util.network.NetworkAddressUtils,<alluxio.util.network.NetworkAddressUtils: java.lang.String getLocalNodeName()>
Control Dependency,alluxio.user.network.netty.channel,alluxio.worker.data.server.domain.socket.as.uuid,alluxio.util.network.NettyUtils,<alluxio.util.network.NettyUtils: boolean isDomainSocketSupported(alluxio.wire.WorkerNetAddress)>
Control Dependency,alluxio.master.mount.table.root.option,alluxio.underfs.s3a.list.objects.v1,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.ObjectUnderFileSystem$ObjectListingChunk getObjectListingChunk(java.lang.String,boolean)>"
Control Dependency,alluxio.master.keytab.file,alluxio.master.mount.table.root.option,alluxio.underfs.hdfs.HdfsUnderFileSystem,<alluxio.underfs.hdfs.HdfsUnderFileSystem: void connectFromMaster(java.lang.String)>
Control Dependency,alluxio.master.mount.table.root.option,alluxio.underfs.s3.proxy.host,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Control Dependency,alluxio.master.mount.table.root.option,aws.secretKey,alluxio.underfs.s3a.S3AUnderFileSystem,<alluxio.underfs.s3a.S3AUnderFileSystem: com.amazonaws.auth.AWSCredentialsProvider createAwsCredentialsProvider(alluxio.underfs.UnderFileSystemConfiguration)>
Control Dependency,alluxio.user.metrics.collection.enabled,alluxio.user.metrics.heartbeat.interval,alluxio.client.file.FileSystemContext,"<alluxio.client.file.FileSystemContext: void init(alluxio.master.MasterInquireClient,alluxio.conf.InstancedConfiguration)>"
Control Dependency,alluxio.master.mount.table.root.option,alluxio.underfs.s3a.signer.algorithm,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Control Dependency,alluxio.master.mount.table.root.option,alluxio.worker.keytab.file,alluxio.underfs.hdfs.HdfsUnderFileSystem,<alluxio.underfs.hdfs.HdfsUnderFileSystem: void connectFromWorker(java.lang.String)>
Control Dependency,alluxio.master.master.heartbeat.interval,alluxio.zookeeper.enabled,alluxio.master.meta.DefaultMetaMaster,<alluxio.master.meta.DefaultMetaMaster: void start(java.lang.Boolean)>
Control Dependency,alluxio.zookeeper.address,alluxio.zookeeper.enabled,alluxio.master.MasterInquireClient$Factory,<alluxio.master.MasterInquireClient$Factory: alluxio.master.MasterInquireClient create(alluxio.AlluxioConfiguration)>
Control Dependency,alluxio.conf.dir,alluxio.home,alluxio.cli.validation.ClusterConfConsistencyValidationTask,<alluxio.cli.validation.ClusterConfConsistencyValidationTask: java.util.Properties getNodeConf(java.lang.String)>
Control Dependency,alluxio.worker.data.server.domain.socket.address,alluxio.worker.network.netty.channel,alluxio.worker.AlluxioWorkerProcess,<alluxio.worker.AlluxioWorkerProcess: boolean isDomainSocketEnabled()>
Control Dependency,alluxio.worker.tieredstore.reserver.enabled,alluxio.worker.tieredstore.reserver.interval,alluxio.worker.block.DefaultBlockWorker,<alluxio.worker.block.DefaultBlockWorker: void start(alluxio.wire.WorkerNetAddress)>
Control Dependency,alluxio.security.authentication.custom.provider.class,alluxio.security.authentication.type,alluxio.security.authentication.AuthenticationProvider,*
Control Dependency,alluxio.master.hostname,alluxio.user.local.writer.packet.size.bytes,alluxio.client.block.stream.LocalFilePacketWriter,"<alluxio.client.block.stream.LocalFilePacketWriter: alluxio.client.block.stream.LocalFilePacketWriter create(alluxio.client.file.FileSystemContext,alluxio.wire.WorkerNetAddress,long,alluxio.client.file.options.OutStreamOptions)>"
Control Dependency,alluxio.locality.order,alluxio.worker.hostname,alluxio.util.network.NetworkAddressUtils,<alluxio.util.network.NetworkAddressUtils: java.lang.String getLocalNodeName()>
Control Dependency,alluxio.zookeeper.enabled,alluxio.zookeeper.leader.path,alluxio.master.MasterInquireClient$Factory,<alluxio.master.MasterInquireClient$Factory: alluxio.master.MasterInquireClient create(alluxio.AlluxioConfiguration)>
Control Dependency,hbase.master.info.port,hbase.master.infoserver.redirect,org.apache.hadoop.hbase.LocalHBaseCluster,"<org.apache.hadoop.hbase.LocalHBaseCluster: void <init>(org.apache.hadoop.conf.Configuration,int,int,java.lang.Class,java.lang.Class)>"
Control Dependency,alluxio.security.authentication.type,alluxio.security.login.username,alluxio.security.LoginUser,*
Control Dependency,alluxio.master.mount.table.root.option,alluxio.underfs.s3.endpoint,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Control Dependency,net.topology.node.switch.mapping.impl,net.topology.table.file.name,org.apache.hadoop.hdfs.ClientContext,<org.apache.hadoop.hdfs.ClientContext: void initTopologyResolution(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.dir,mapreduce.task.files.preserve.failedtasks,org.apache.hadoop.mapreduce.v2.app.MRAppMaster,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void cleanupStagingDir()>
Control Dependency,mapreduce.job.dir,mapreduce.task.files.preserve.filepattern,org.apache.hadoop.mapred.Task,"<org.apache.hadoop.mapred.Task: void runJobCleanupTask(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
Control Dependency,mapreduce.map.input.length,mapreduce.map.input.start,org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper,<org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper: boolean fileSplitIsValid(org.apache.hadoop.mapreduce.TaskAttemptContext)>
Control Dependency,mapreduce.output.fileoutputformat.compress,mapreduce.output.fileoutputformat.compress.type,org.apache.hadoop.mapred.MapFileOutputFormat,"<org.apache.hadoop.mapred.MapFileOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>"
Control Dependency,mapreduce.input.multipleinputs.dir.formats,mapreduce.job.working.dir,org.apache.hadoop.mapred.JobConf,<org.apache.hadoop.mapred.JobConf: org.apache.hadoop.fs.Path getWorkingDirectory()>
Control Dependency,mapreduce.ifile.readahead,mapreduce.ifile.readahead.bytes,org.apache.hadoop.mapred.IFileInputStream,<org.apache.hadoop.mapred.IFileInputStream: void doReadahead()>
Control Dependency,mapreduce.job.cache.files,mapreduce.job.cache.limit.max-resources,org.apache.hadoop.mapreduce.JobResourceUploader,"<org.apache.hadoop.mapreduce.JobResourceUploader: void checkLocalizationLimits(org.apache.hadoop.conf.Configuration,java.util.Collection,java.util.Collection,java.util.Collection,java.lang.String,java.util.Map)>"
Control Dependency,mapreduce.task.profile,mapreduce.task.profile.maps,org.apache.hadoop.mapred.MapReduceChildJVM,"<org.apache.hadoop.mapred.MapReduceChildJVM: java.util.List getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId)>"
Control Dependency,mapreduce.reduce.memory.mb,mapreduce.task.attempt.id,org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: int getMemoryRequired(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.api.records.TaskType)>"
Control Dependency,mapreduce.job.classloader,mapreduce.job.classloader.system.classes,org.apache.hadoop.mapreduce.v2.util.MRApps,<org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.String[] getSystemClasses(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.reduces,mapreduce.local.reduce.tasks.maximum,org.apache.hadoop.mapred.LocalJobRunner$Job,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createReduceExecutor()>
Control Dependency,mapreduce.job.skip.outdir,mapreduce.output.fileoutputformat.outputdir,org.apache.hadoop.mapred.FileOutputFormat,<org.apache.hadoop.mapred.FileOutputFormat: org.apache.hadoop.fs.Path getOutputPath(org.apache.hadoop.mapred.JobConf)>
Control Dependency,mapreduce.map.speculative,mapreduce.reduce.speculative,org.apache.hadoop.mapreduce.v2.app.MRAppMaster,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.output.fileoutputformat.compress,mapreduce.output.fileoutputformat.compress.codec,org.apache.hadoop.mapred.MapFileOutputFormat,"<org.apache.hadoop.mapred.MapFileOutputFormat: org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)>"
Control Dependency,mapreduce.job.encrypted-intermediate-data,mapreduce.job.encrypted-intermediate-data-key-size-bits,org.apache.hadoop.mapreduce.v2.app.MRAppMaster,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void initJobCredentialsAndUGI(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.randomwriter.bytespermap,mapreduce.randomwriter.totalbytes,org.apache.hadoop.examples.RandomWriter,<org.apache.hadoop.examples.RandomWriter: int run(java.lang.String[])>
Control Dependency,mapreduce.jobhistory.loadedjobs.cache.size,mapreduce.jobhistory.loadedtasks.cache.size,org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$2,*
Control Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxbytes,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Control Dependency,mapreduce.jobhistory.intermediate-done-dir,mapreduce.jobhistory.maximum-start-wait-time-millis,org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils,<org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: java.lang.String getConfiguredHistoryIntermediateDoneDirPrefix(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.cache.archives,mapreduce.job.cache.limit.max-resources,org.apache.hadoop.mapreduce.JobResourceUploader,"<org.apache.hadoop.mapreduce.JobResourceUploader: void checkLocalizationLimits(org.apache.hadoop.conf.Configuration,java.util.Collection,java.util.Collection,java.util.Collection,java.lang.String,java.util.Map)>"
Control Dependency,mapreduce.framework.name,mapreduce.jobtracker.kerberos.principal,org.apache.hadoop.mapred.Master,<org.apache.hadoop.mapred.Master: java.lang.String getMasterPrincipal(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.map.input.file,mapreduce.map.input.length,org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper,<org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper: boolean fileSplitIsValid(org.apache.hadoop.mapreduce.TaskAttemptContext)>
Control Dependency,mapreduce.fileoutputcommitter.algorithm.version,mapreduce.fileoutputcommitter.failures.attempts,org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
Control Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxreduces,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Control Dependency,mapreduce.map.memory.mb,mapreduce.task.attempt.id,org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: int getMemoryRequired(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.api.records.TaskType)>"
Control Dependency,mapreduce.map.output.compress,mapreduce.map.output.compress.codec,org.apache.hadoop.mapred.JobConf,*
Control Dependency,mapreduce.client.submit.file.replication,mapreduce.job.log4j-properties-file,org.apache.hadoop.mapreduce.JobResourceUploader,"<org.apache.hadoop.mapreduce.JobResourceUploader: void addLog4jToDistributedCache(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>"
Control Dependency,mapreduce.jobhistory.cleaner.enable,mapreduce.jobhistory.cleaner.interval-ms,org.apache.hadoop.mapreduce.v2.hs.JobHistory,<org.apache.hadoop.mapreduce.v2.hs.JobHistory: void scheduleHistoryCleaner()>
Control Dependency,mapreduce.reduce.cpu.vcores,mapreduce.task.attempt.id,org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: int getCpuRequired(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.api.records.TaskType)>"
Control Dependency,yarn.app.mapreduce.am.profile,yarn.app.mapreduce.am.profile.params,org.apache.hadoop.mapred.YARNRunner,<org.apache.hadoop.mapred.YARNRunner: java.util.List setupAMCommand(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxmaps,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Control Dependency,dfs.encrypt.data.transfer.cipher.key.bitlength,dfs.encrypt.data.transfer.cipher.suites,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.crypto.CipherOption negotiateCipherOption(org.apache.hadoop.conf.Configuration,java.util.List)>"
Control Dependency,yarn.resourcemanager.fs.state-store.uri,yarn.resourcemanager.store.class,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,*
Control Dependency,hadoop.security.group.mapping.ldap.ssl.truststore.password,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,zk-dt-secret-manager.kerberos.principal,zk-dt-secret-manager.zkAuthType,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: java.lang.String setJaasConfiguration(org.apache.hadoop.conf.Configuration)>
Control Dependency,io.compression.codec.bzip2.library,io.native.lib.available,org.apache.hadoop.io.compress.bzip2.Bzip2Factory,<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.client.block.write.replace-datanode-on-failure.enable,dfs.client.block.write.replace-datanode-on-failure.policy,org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure,<org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure: org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy getPolicy(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.azure.authorization,fs.azure.enable.spnego.token.cache,org.apache.hadoop.fs.azure.NativeAzureFileSystem,"<org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>"
Control Dependency,dfs.client.read.shortcircuit,dfs.client.read.shortcircuit.streams.cache.size,org.apache.hadoop.hdfs.client.impl.BlockReaderFactory,*
Control Dependency,dfs.namenode.lifeline.handler.count,dfs.namenode.lifeline.rpc-address,org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer,*
Control Dependency,hadoop.rpc.protection,hadoop.security.saslproperties.resolver.class,org.apache.hadoop.conf.Configuration,"<org.apache.hadoop.conf.Configuration: void putIntoUpdatingResource(java.lang.String,java.lang.String[])>"
Control Dependency,yarn.nodemanager.resource.cpu-vcores,yarn.nodemanager.resource.pcores-vcores-multiplier,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getVCoresInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Control Dependency,yarn.nodemanager.resource.detect-hardware-capabilities,yarn.nodemanager.resource.pcores-vcores-multiplier,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getVCoresInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Control Dependency,dfs.client.read.shortcircuit,dfs.client.read.shortcircuit.streams.cache.expiry.ms,org.apache.hadoop.hdfs.client.impl.BlockReaderFactory,*
Control Dependency,yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,org.apache.hadoop.hdfs.DataStreamer,*
Control Dependency,yarn.nodemanager.resource.count-logical-processors-as-cores,yarn.nodemanager.resource.detect-hardware-capabilities,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getVCoresInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Control Dependency,yarn.resourcemanager.ha.automatic-failover.embedded,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: boolean isAutomaticFailoverEmbedded(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.group.mapping.ldap.ssl,hadoop.security.group.mapping.ldap.ssl.keystore,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.client.failover-max-attempts,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Control Dependency,hadoop.security.group.mapping.ldap.ssl.keystore.password,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.group.mapping.ldap.ssl.truststore,hadoop.security.group.mapping.ldap.url,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,io.bytes.per.checksum,io.skip.checksum.errors,org.apache.hadoop.io.SequenceFile$Reader,<org.apache.hadoop.io.SequenceFile$Reader: void handleChecksumException(org.apache.hadoop.fs.ChecksumException)>
Control Dependency,yarn.resourcemanager.ha.automatic-failover.enabled,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: boolean isAutomaticFailoverEmbedded(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.sasl.variablewhitelist.cache.secs,hadoop.security.sasl.variablewhitelist.enable,org.apache.hadoop.security.WhitelistBasedResolver,<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.webhdfs.rest-csrf.enabled,dfs.webhdfs.rest-csrf.methods-to-ignore,org.apache.hadoop.hdfs.web.WebHdfsFileSystem,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void initializeRestCsrf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.client.read.shortcircuit,dfs.client.read.shortcircuit.buffer.size,org.apache.hadoop.hdfs.client.impl.BlockReaderFactory,*
Control Dependency,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,hadoop.security.group.mapping.ldap.url,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.webhdfs.rest-csrf.custom-header,dfs.webhdfs.rest-csrf.enabled,org.apache.hadoop.hdfs.web.WebHdfsFileSystem,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void initializeRestCsrf(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.sasl.variablewhitelist.enable,hadoop.security.sasl.variablewhitelist.file,org.apache.hadoop.security.WhitelistBasedResolver,<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.devicecode.clientapp.id,org.apache.hadoop.fs.adl.AdlFileSystem,<org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getDeviceCodeTokenProvider(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.store.class,yarn.resourcemanager.zk-state-store.parent-path,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,*
Control Dependency,hadoop.security.credential.provider.path,hadoop.security.kms.client.failover.max.retries,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider,"<org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: java.lang.Object doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int)>"
Control Dependency,mapreduce.task.profile,mapreduce.task.profile.reduces,org.apache.hadoop.mapred.MapReduceChildJVM,"<org.apache.hadoop.mapred.MapReduceChildJVM: java.util.List getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId)>"
Control Dependency,io.compression.codec.bzip2.library,io.file.buffer.size,org.apache.hadoop.io.compress.BZip2Codec,"<org.apache.hadoop.io.compress.BZip2Codec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>"
Control Dependency,hadoop.security.group.mapping.ldap.ssl.keystore.password,hadoop.security.group.mapping.ldap.url,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.ssl.enabled,hadoop.ssl.enabled.protocols,org.apache.hadoop.http.HttpServer,*
Control Dependency,dfs.balancer.keytab.enabled,dfs.balancer.keytab.file,org.apache.hadoop.hdfs.server.balancer.Balancer,*
Control Dependency,rpc.metrics.percentiles.intervals,rpc.metrics.quantile.enable,org.apache.hadoop.ipc.metrics.RpcMetrics,*
Control Dependency,hadoop.security.dns.log-slow-lookups.enabled,hadoop.security.dns.log-slow-lookups.threshold.ms,"""org.apache.hadoop.security.SecurityUtil",*
Control Dependency,hadoop.registry.jaas.context,hadoop.registry.secure,org.apache.hadoop.registry.client.impl.zk.RegistrySecurity,<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void initSecurity()>
Control Dependency,dfs.namenode.replication.considerLoad,dfs.namenode.replication.considerLoad.factor,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: boolean isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)>"
Control Dependency,hadoop.http.logs.enabled,hadoop.jetty.logs.serve.aliases,org.apache.hadoop.http.HttpServer2,"<org.apache.hadoop.http.HttpServer2: void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)>"
Control Dependency,hadoop.security.authorization,hadoop.security.instrumentation.requires.admin,org.apache.hadoop.http.HttpServer,"<org.apache.hadoop.http.HttpServer: boolean hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Control Dependency,yarn.nodemanager.resource.memory-mb,yarn.nodemanager.resource.system-reserved-memory-mb,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getContainerMemoryMBInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Control Dependency,dfs.client.read.shortcircuit,dfs.client.read.shortcircuit.skip.checksum,org.apache.hadoop.hdfs.client.impl.BlockReaderFactory,*
Control Dependency,ipc.client.ping,ipc.ping.interval,org.apache.hadoop.ipc.Client,<org.apache.hadoop.ipc.Client: int getPingInterval(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.datanode.sync.behind.writes,dfs.datanode.sync.behind.writes.in.background,org.apache.hadoop.hdfs.server.datanode.BlockReceiver,*
Control Dependency,dfs.namenode.service.handler.count,dfs.namenode.servicerpc-address,org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer,*
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.msi.port,org.apache.hadoop.fs.adl.AdlFileSystem,<org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getMsiBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.lifeline.handler.count,dfs.namenode.lifeline.handler.ratio,org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer,*
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.credential,org.apache.hadoop.fs.adl.AdlFileSystem,<org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getConfCredentialBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.refresh.token,org.apache.hadoop.fs.adl.AdlFileSystem,<org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getConfRefreshTokenBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.client.read.shortcircuit,dfs.domain.socket.path,org.apache.hadoop.hdfs.server.datanode.DataNode,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.net.DomainPeerServer getDomainPeerServer(org.apache.hadoop.conf.Configuration,int)>"
Control Dependency,hadoop.rpc.protection.non-whitelist,hadoop.security.saslproperties.resolver.class,,*
Control Dependency,yarn.client.failover-sleep-base-ms,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Control Dependency,ipc.client.ping,ipc.client.rpc-timeout.ms,org.apache.hadoop.ipc.Client,<org.apache.hadoop.ipc.Client: int getTimeout(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.azure.local.sas.key.mode,fs.azure.secure.mode,org.apache.hadoop.fs.azure.NativeAzureFileSystem,"<org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>"
Control Dependency,yarn.resourcemanager.scheduler.monitor.enable,yarn.resourcemanager.scheduler.monitor.policies,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices: void createSchedulerMonitors()>
Control Dependency,yarn.nodemanager.resource.detect-hardware-capabilities,yarn.nodemanager.resource.system-reserved-memory-mb,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getContainerMemoryMBInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Control Dependency,fs.s3a.fast.upload,fs.s3a.fast.upload.buffer,org.apache.hadoop.fs.s3a.S3AFileSystem,"<org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>"
Control Dependency,yarn.client.failover-sleep-max-ms,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Control Dependency,yarn.nodemanager.linux-container-executor.cgroups.mount,yarn.nodemanager.linux-container-executor.cgroups.mount-path,org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler,"<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin)>"
Control Dependency,yarn.nodemanager.recovery.dir,yarn.nodemanager.recovery.enabled,org.apache.hadoop.yarn.server.nodemanager.NodeManager,<org.apache.hadoop.yarn.server.nodemanager.NodeManager: void initAndStartRecoveryStore(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.acl.enable,yarn.acl.reservation-enable,org.apache.hadoop.yarn.server.resourcemanager.security.ReservationsACLsManager,*
Control Dependency,fs.s3n.multipart.uploads.block.size,fs.s3n.multipart.uploads.enabled,org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore,"<org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore: void storeFile(java.lang.String,java.io.File,byte[])>"
Control Dependency,hadoop.security.group.mapping.ldap.bind.password,hadoop.security.group.mapping.ldap.bind.password.file,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.s3a.multipart.purge,fs.s3a.multipart.purge.age,org.apache.hadoop.fs.s3a.S3AFileSystem,*
Control Dependency,zk-dt-secret-manager.kerberos.keytab,zk-dt-secret-manager.zkAuthType,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: java.lang.String setJaasConfiguration(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.timeline-service.recovery.enabled,yarn.timeline-service.state-store-class,org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService,<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.acl.enable,yarn.resourcemanager.webapp.rest-csrf.enabled,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: java.lang.String getCSRFHeaderString(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.runtime.linux.docker.default-container-network,yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void initialize(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.runtime.linux.docker.default-container-network,yarn.nodemanager.runtime.linux.docker.privileged-containers.acl,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void initialize(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.ha.automatic-failover.embedded,yarn.resourcemanager.ha.curator-leader-elector.enabled,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElector createEmbeddedElector()>
Control Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.client.util.YarnClientUtils,<org.apache.hadoop.yarn.client.util.YarnClientUtils: org.apache.hadoop.yarn.conf.YarnConfiguration getYarnConfWithRmHaId(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.ha.rm-ids,yarn.resourcemanager.principal,org.apache.hadoop.yarn.util.RMHAUtils,<org.apache.hadoop.yarn.util.RMHAUtils: org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getHAState(org.apache.hadoop.yarn.conf.YarnConfiguration)>
Control Dependency,yarn.timeline-service.address,yarn.timeline-service.generic-application-history.enabled,org.apache.hadoop.yarn.server.webproxy.AppReportFetcher,<org.apache.hadoop.yarn.server.webproxy.AppReportFetcher: org.apache.hadoop.yarn.api.ApplicationHistoryProtocol getAHSProxy(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.timeline-service.generic-application-history.enabled,yarn.timeline-service.generic-application-history.store-class,org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter,<org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter: org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore createApplicationHistoryStore(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.timeline-service.generic-application-history.enabled,yarn.timeline-service.version,org.apache.hadoop.yarn.client.api.impl.YarnClientImpl,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.node-labels.provider,yarn.nodemanager.node-labels.provider.configured-node-partition,,*
Control Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.webapp.https.address,org.apache.hadoop.yarn.webapp.util.WebAppUtils,"<org.apache.hadoop.yarn.webapp.util.WebAppUtils: java.lang.String getRMWebAppURLWithoutScheme(org.apache.hadoop.conf.Configuration,boolean)>"
Control Dependency,yarn.scheduler.configuration.store.class,yarn.scheduler.configuration.store.max-logs,,*
Control Dependency,yarn.federation.enabled,yarn.resourcemanager.epoch,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices: void serviceStart()>
Control Dependency,yarn.nodemanager.distributed-scheduling.enabled,yarn.resourcemanager.scheduler.client.thread-count,org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService,"<org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService: org.apache.hadoop.ipc.Server getServer(org.apache.hadoop.yarn.ipc.YarnRPC,org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager)>"
Control Dependency,yarn.resourcemanager.address,yarn.resourcemanager.zk-state-store.root-node.acl,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore,"<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: java.util.List constructZkRootNodeACL(org.apache.hadoop.conf.Configuration,java.util.List)>"
Control Dependency,yarn.nodemanager.runtime.linux.docker.capabilities,yarn.nodemanager.runtime.linux.docker.default-container-network,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: java.util.Set getDockerCapabilitiesFromConf()>
Control Dependency,yarn.sharedcache.enabled,yarn.sharedcache.uploader.server.address,org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService,<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService: org.apache.hadoop.yarn.server.api.SCMUploaderProtocol createSCMClient(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.sharedcache.enabled,yarn.sharedcache.nm.uploader.thread-count,org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService,<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.configuration.file-system-based-store,yarn.resourcemanager.configuration.provider-class,,*
Control Dependency,yarn.log-aggregation-enable,yarn.log.server.url,org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebAppFilter,<org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebAppFilter: java.lang.String containerLogPageRedirectPath(javax.servlet.http.HttpServletRequest)>
Control Dependency,yarn.nodemanager.node-labels.provider,yarn.nodemanager.node-labels.provider.fetch-interval-ms,,*
Control Dependency,yarn.nodemanager.health-checker.script.opts,yarn.nodemanager.health-checker.script.path,org.apache.hadoop.yarn.server.nodemanager.NodeManager,<org.apache.hadoop.yarn.server.nodemanager.NodeManager: org.apache.hadoop.util.NodeHealthScriptRunner getNodeHealthScriptRunner(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size,yarn.timeline-service.generic-application-history.enabled,org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter,<org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter: org.apache.hadoop.yarn.event.Dispatcher createDispatcher(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,yarn.scheduler.configuration.store.class,,*
Control Dependency,yarn.nodemanager.recovery.enabled,yarn.nodemanager.recovery.supervised,org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl,<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: boolean isNMUnderSupervisionWithRecoveryEnabled()>
Control Dependency,yarn.resourcemanager.ha.automatic-failover.enabled,yarn.resourcemanager.ha.curator-leader-elector.enabled,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: void verifyLeaderElection(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.amrmproxy.enabled,yarn.nodemanager.distributed-scheduling.enabled,org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void createAMRMProxyService(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.address,yarn.resourcemanager.ha.curator-leader-elector.enabled,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.util.curator.ZKCuratorManager createAndStartZKManager(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.shuffle.transfer.buffer.size,mapreduce.shuffle.transferto.allowed,*,*
Control Dependency,yarn.acl.enable,yarn.resourcemanager.webapp.rest-csrf.,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: java.lang.String getCSRFHeaderString(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.log-aggregation-enable,yarn.log-aggregation.retain-seconds,org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService,<org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: void scheduleLogDeletionTask()>
Control Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.webapp.address,org.apache.hadoop.yarn.webapp.util.WebAppUtils,"<org.apache.hadoop.yarn.webapp.util.WebAppUtils: java.lang.String getRMWebAppURLWithoutScheme(org.apache.hadoop.conf.Configuration,boolean)>"
Control Dependency,yarn.scheduler.configuration.leveldb-store.path,yarn.scheduler.configuration.store.class,,*
Control Dependency,yarn.resourcemanager.recovery.enabled,yarn.resourcemanager.work-preserving-recovery.enabled,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.runtime.linux.docker.default-container-network,yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void initialize(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.webapp.ui2.enable,yarn.webapp.ui2.war-file-path,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void startWepApp()>
Control Dependency,yarn.nodemanager.runtime.linux.docker.allowed-container-networks,yarn.nodemanager.runtime.linux.docker.capabilities,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: java.util.Set getDockerCapabilitiesFromConf()>
Control Dependency,yarn.resourcemanager.reservation-system.class,yarn.resourcemanager.reservation-system.enable,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystem createReservationSystem()>
Control Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.zk-state-store.root-node.acl,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore,<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void initInternal(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.health-checker.interval-ms,yarn.nodemanager.health-checker.script.path,org.apache.hadoop.yarn.server.nodemanager.NodeManager,<org.apache.hadoop.yarn.server.nodemanager.NodeManager: org.apache.hadoop.util.NodeHealthScriptRunner getNodeHealthScriptRunner(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.ha.id,org.apache.hadoop.yarn.client.util.YarnClientUtils,<org.apache.hadoop.yarn.client.util.YarnClientUtils: org.apache.hadoop.yarn.conf.YarnConfiguration getYarnConfWithRmHaId(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.ha.curator-leader-elector.enabled,yarn.resourcemanager.zk-state-store.root-node.acl,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.util.curator.ZKCuratorManager createAndStartZKManager(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.address,yarn.resourcemanager.principal,org.apache.hadoop.yarn.client.util.YarnClientUtils,"<org.apache.hadoop.yarn.client.util.YarnClientUtils: java.lang.String getRmPrincipal(java.lang.String,org.apache.hadoop.conf.Configuration)>"
Control Dependency,yarn.resourcemanager.ha.rm-ids,yarn.web-proxy.address,org.apache.hadoop.yarn.util.RMHAUtils,<org.apache.hadoop.yarn.util.RMHAUtils: java.util.List getRMHAWebappAddresses(org.apache.hadoop.yarn.conf.YarnConfiguration)>
Control Dependency,yarn.resourcemanager.webapp.rest-csrf.,yarn.resourcemanager.webapp.rest-csrf.enabled,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: java.lang.String getCSRFHeaderString(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.node-labels.configuration-type,yarn.resourcemanager.node-labels.provider,,*
Control Dependency,yarn.nodemanager.node-labels.provider,yarn.nodemanager.node-labels.provider.fetch-timeout-ms,,*
Control Dependency,yarn.resourcemanager.ha.id,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getRMHAId(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.nodemanager.runtime.linux.docker.default-container-network,yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void initialize(org.apache.hadoop.conf.Configuration)>
Control Dependency,yarn.resourcemanager.ha.automatic-failover.embedded,yarn.resourcemanager.ha.automatic-failover.enabled,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: boolean isAutomaticFailoverEmbedded(org.apache.hadoop.conf.Configuration)>
Control Dependency,isReconfigEnabled,quorumVerifier,org.apache.zookeeper.server.quorum.QuorumPeerConfig,<org.apache.zookeeper.server.quorum.QuorumPeerConfig: void parseProperties(java.util.Properties)>
Control Dependency,electiontype,cnxtimeout,org.apache.zookeeper.server.quorum.QuorumPeer,<org.apache.zookeeper.server.quorum.QuorumPeer: org.apache.zookeeper.server.quorum.QuorumCnxManager createCnxnManager()>
Control Dependency,zookeeper.emulate353TTLNodes,zookeeper.extendedTypesEnabled,org.apache.zookeeper.server.EphemeralType,<org.apache.zookeeper.server.EphemeralType: org.apache.zookeeper.server.EphemeralType get(long)>
Control Dependency,getLastSeenQuorumVerifier,getQuorumVerifier,org.apache.zookeeper.server.quorum.Leader,<org.apache.zookeeper.server.quorum.Leader: org.apache.zookeeper.server.quorum.Leader$Proposal propose(org.apache.zookeeper.server.Request)>
Control Dependency,getQuorumVerifier,isReconfigEnabled,org.apache.zookeeper.server.quorum.Learner,<org.apache.zookeeper.server.quorum.Learner: void syncWithLeader(long)>
Control Dependency,getQuorumVerifier,getServerId,org.apache.zookeeper.server.quorum.Leader,"<org.apache.zookeeper.server.quorum.Leader: long getDesignatedLeader(org.apache.zookeeper.server.quorum.Leader$Proposal,long)>"
Control Dependency,dfs.client.domain.socket.data.traffic,dfs.client.read.shortcircuit,,*
Control Dependency,dfs.web.authentication.filter,dfs.webhdfs.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.http.HttpServer2,java.lang.String)>"
Control Dependency,dfs.datatransfer.client.variablewhitelist.cache.secs,dfs.datatransfer.client.variablewhitelist.enable,org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver,<org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.webhdfs.acl.provider.permission.pattern,dfs.webhdfs.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.http.HttpServer2,java.lang.String)>"
Control Dependency,dfs.block.replicator.classname,dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction,,*
Control Dependency,dfs.web.authentication.simple.anonymous.allowed,dfs.webhdfs.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Control Dependency,dfs.federation.router.default.nameserviceId,dfs.nameservices,org.apache.hadoop.hdfs.DFSUtilClient,<org.apache.hadoop.hdfs.DFSUtilClient: java.util.Collection getNameServiceIds(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.image.compress,dfs.image.compression.codec,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,<org.apache.hadoop.hdfs.server.namenode.FSImageCompression: org.apache.hadoop.hdfs.server.namenode.FSImageCompression createCompression(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.lifeline.handler.ratio,dfs.namenode.lifeline.rpc-address,*,*
Control Dependency,dfs.namenode.edits.dir,dfs.namenode.edits.dir.required,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection getRequiredNamespaceEditsDirs(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.federation.router.heartbeat.enable,dfs.federation.router.monitor.localnamenode.enable,org.apache.hadoop.hdfs.server.federation.router.Router,<org.apache.hadoop.hdfs.server.federation.router.Router: java.util.Collection createNamenodeHeartbeatServices()>
Control Dependency,dfs.block.access.token.enable,dfs.encrypt.data.transfer.algorithm,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.balancer.address,dfs.balancer.keytab.enabled,org.apache.hadoop.hdfs.server.balancer.Balancer,<org.apache.hadoop.hdfs.server.balancer.Balancer: void checkKeytabAndInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.client.failover.max.attempts,dfs.client.test.drop.namenode.response.number,org.apache.hadoop.hdfs.NameNodeProxiesClient,"<org.apache.hadoop.hdfs.NameNodeProxiesClient: org.apache.hadoop.hdfs.NameNodeProxiesClient$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>"
Control Dependency,dfs.namenode.max.op.size,dfs.namenode.startup,org.apache.hadoop.hdfs.server.namenode.FSImage,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Control Dependency,dfs.ha.namenode.id,dfs.nameservices,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getNameServiceId(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Control Dependency,dfs.web.authentication.kerberos.principal,dfs.webhdfs.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Control Dependency,dfs.datatransfer.client.variablewhitelist.enable,dfs.datatransfer.client.variablewhitelist.file,org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver,<org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.access.token.provider,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.net.URI getInfoServer(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,java.lang.String)>"
Control Dependency,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.background.reload,org.apache.zookeeper.ClientCnxn$SendThread,"<org.apache.zookeeper.ClientCnxn$SendThread: void onConnected(int,long,byte[],boolean)>"
Control Dependency,dfs.datatransfer.server.variablewhitelist.enable,dfs.datatransfer.server.variablewhitelist.file,org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver,<org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.keytab.file,dfs.webhdfs.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Control Dependency,dfs.namenode.checkpoint.period,dfs.namenode.startup,org.apache.hadoop.hdfs.server.namenode.FSImage,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean needsResaveBasedOnStaleCheckpoint(java.io.File,long)>"
Control Dependency,dfs.webhdfs.enabled,dfs.webhdfs.rest-csrf.enabled,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.http.HttpServer2,java.lang.String)>"
Control Dependency,dfs.webhdfs.rest-csrf.,dfs.webhdfs.rest-csrf.enabled,org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer,<org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: org.apache.hadoop.security.http.RestCsrfPreventionFilter createRestCsrfPreventionFilter(org.apache.hadoop.conf.Configuration)>
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.client.id,*,*
Control Dependency,dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,dfs.datanode.fsdataset.volume.choosing.policy,*,*
Control Dependency,dfs.datatransfer.server.variablewhitelist.cache.secs,dfs.datatransfer.server.variablewhitelist.enable,org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver,<org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver: void setConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.block.access.key.update.interval,dfs.block.access.token.enable,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.kerberos.principal,dfs.namenode.rpc-address,org.apache.hadoop.hdfs.server.namenode.ImageServlet,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
Control Dependency,dfs.namenode.startup,dfs.reformat.disabled,org.apache.hadoop.hdfs.server.namenode.NameNode,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Control Dependency,dfs.block.access.token.enable,dfs.namenode.rpc-address,org.apache.hadoop.hdfs.DFSUtil,<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getNamenodeNameServiceId(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.shared.edits.dir,dfs.namenode.startup,org.apache.hadoop.hdfs.server.namenode.NameNode,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Control Dependency,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,dfs.datanode.fsdataset.volume.choosing.policy,*,*
Control Dependency,dfs.ha.automatic-failover.enabled,dfs.ha.zkfc.port,org.apache.hadoop.hdfs.tools.DFSZKFailoverController,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: int getZkfcPort(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.checkpoint.txns,dfs.namenode.startup,org.apache.hadoop.hdfs.server.namenode.FSImage,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean needsResaveBasedOnStaleCheckpoint(java.io.File,long)>"
Control Dependency,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.refresh.url,*,*
Control Dependency,dfs.webhdfs.enabled,dfs.webhdfs.user.provider.user.pattern,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.http.HttpServer2,java.lang.String)>"
Control Dependency,dfs.federation.router.heartbeat.enable,dfs.federation.router.monitor.namenode,org.apache.hadoop.hdfs.server.federation.router.Router,<org.apache.hadoop.hdfs.server.federation.router.Router: java.util.Collection createNamenodeHeartbeatServices()>
Control Dependency,dfs.client.failover.sleep.max.millis,dfs.client.test.drop.namenode.response.number,org.apache.hadoop.hdfs.NameNodeProxiesClient,"<org.apache.hadoop.hdfs.NameNodeProxiesClient: org.apache.hadoop.hdfs.NameNodeProxiesClient$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>"
Control Dependency,dfs.block.access.token.enable,dfs.block.access.token.lifetime,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.client.failover.sleep.base.millis,dfs.client.test.drop.namenode.response.number,org.apache.hadoop.hdfs.NameNodeProxiesClient,"<org.apache.hadoop.hdfs.NameNodeProxiesClient: org.apache.hadoop.hdfs.NameNodeProxiesClient$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>"
Value Relationship Dependency,dfs.cachereport.intervalMsec,dfs.datanode.max.locked.memory,org.apache.hadoop.hdfs.server.datanode.BPServiceActor,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: DatanodeCommand cacheReport()>
Value Relationship Dependency,hbase.hstore.compaction.min.size,hbase.hstore.compaction.ratio,org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy,"<org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy: java.util.ArrayList applyCompactionPolicy(java.util.ArrayList,boolean,boolean)>"
Value Relationship Dependency,hbase.hregion.max.filesize,hbase.hregion.memstore.flush.size,org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy,<org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy: long getSizeToCheck(int)>
Value Relationship Dependency,hbase.regionserver.hostname,hbase.regionserver.hostname.disable.master.reversedns,org.apache.hadoop.hbase.regionserver.HRegionServer,<org.apache.hadoop.hbase.regionserver.HRegionServer: void handleReportForDutyResponse(org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStartupResponse)>
Value Relationship Dependency,hbase.client.scanner.timeout.period,hbase.rpc.timeout,org.apache.hadoop.hbase.regionserver.RSRpcServices,"<org.apache.hadoop.hbase.regionserver.RSRpcServices: long getTimeLimit(org.apache.hadoop.hbase.ipc.HBaseRpcController,boolean)>"
Value Relationship Dependency,hbase.hregion.memstore.flush.size,hbase.hregion.percolumnfamilyflush.size.lower.bound.min,org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy,<org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy: void setFlushSizeLowerBounds(org.apache.hadoop.hbase.regionserver.HRegion)>
Value Relationship Dependency,hbase.client.pause,hbase.client.pause.cqtbe,org.apache.hadoop.hbase.client.AsyncProcess,"<org.apache.hadoop.hbase.client.AsyncProcess: void <init>(org.apache.hadoop.hbase.client.ClusterConnection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory)>"
Value Relationship Dependency,spark.executor.heartbeatInterval,spark.network.timeout,org.apache.spark.serializer.KryoSerializer,*
Value Relationship Dependency,spark.r.backendConnectionTimeout,spark.r.heartBeatInterval,org.apache.spark.api.r.RBackendHandler,"<org.apache.spark.api.r.RBackendHandler: void channelRead0(io.netty.channel.ChannelHandlerContext,byte[])>"
Value Relationship Dependency,spark.rpc.message.maxSize,spark.shuffle.mapOutput.minSizeForBroadcast,org.apache.spark.MapOutputTrackerMaster,"<org.apache.spark.MapOutputTrackerMaster: void <init>(org.apache.spark.SparkConf,org.apache.spark.broadcast.BroadcastManager,boolean)>"
Value Relationship Dependency,spark.rpc.message.maxSize,spark.task.maxDirectResultSize,org.apache.spark.executor.Executor,"<org.apache.spark.executor.Executor: void <init>(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean,java.lang.Thread$UncaughtExceptionHandler)>"
Value Relationship Dependency,spark.kryoserializer.buffer,spark.kryoserializer.buffer.max,org.apache.spark.serializer.KryoSerializer,*
Value Relationship Dependency,spark.memory.offHeap.enabled,spark.memory.offHeap.size,org.apache.spark.memory.MemoryManager,"<org.apache.spark.memory.MemoryManager: void <init>(org.apache.spark.SparkConf,int,long,long)>"
Value Relationship Dependency,alluxio.master.worker.threads.max,alluxio.master.worker.threads.min,alluxio.master.AlluxioMasterProcess,<alluxio.master.AlluxioMasterProcess: void <init>(alluxio.master.journal.JournalSystem)>
Value Relationship Dependency,alluxio.master.worker.connect.wait.time,alluxio.user.rpc.retry.max.sleep,alluxio.conf.InstancedConfiguration,<alluxio.conf.InstancedConfiguration: void checkTimeouts()>
Value Relationship Dependency,alluxio.underfs.s3.threads.max,alluxio.underfs.s3.upload.threads.max,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Value Relationship Dependency,alluxio.worker.data.server.domain.socket.address,alluxio.worker.data.server.domain.socket.as.uuid,alluxio.worker.AlluxioWorkerProcess,<alluxio.worker.AlluxioWorkerProcess: void <init>(alluxio.wire.TieredIdentity)>
Value Relationship Dependency,alluxio.underfs.s3.admin.threads.max,alluxio.underfs.s3.threads.max,alluxio.underfs.s3a.S3AUnderFileSystem,"<alluxio.underfs.s3a.S3AUnderFileSystem: alluxio.underfs.s3a.S3AUnderFileSystem createInstance(alluxio.AlluxioURI,alluxio.underfs.UnderFileSystemConfiguration)>"
Value Relationship Dependency,mapreduce.job.speculative.minimum-allowed-tasks,mapreduce.job.speculative.speculative-cap-total-tasks,org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: int maybeScheduleASpeculation(org.apache.hadoop.mapreduce.v2.api.records.TaskType)>
Value Relationship Dependency,mapreduce.job.end-notification.max.attempts,mapreduce.job.end-notification.retry.attempts,org.apache.hadoop.mapreduce.v2.app.JobEndNotifier,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void setConf(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,mapreduce.jobhistory.recovery.enable,mapreduce.jobhistory.recovery.store.class,org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreServiceFactory,*
Value Relationship Dependency,mapreduce.job.end-notification.max.retry.interval,mapreduce.job.end-notification.retry.interval,org.apache.hadoop.mapreduce.v2.app.JobEndNotifier,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void setConf(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,mapreduce.application.classpath,mapreduce.application.framework.path,org.apache.hadoop.mapreduce.v2.util.MRApps,"<org.apache.hadoop.mapreduce.v2.util.MRApps: void setMRFrameworkClasspath(java.util.Map,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,yarn.am.liveness-monitor.expiry-interval-ms,yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs,org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager,"<org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)>"
Value Relationship Dependency,hadoop.registry.zk.retry.ceiling.ms,hadoop.registry.zk.retry.interval.ms,org.apache.curator.retry.BoundedExponentialBackoffRetry,"<org.apache.curator.retry.BoundedExponentialBackoffRetry: int getSleepTimeMs(int,long)>"
Value Relationship Dependency,dfs.datanode.lifeline.interval.seconds,dfs.heartbeat.interval,org.apache.hadoop.hdfs.server.datanode.DNConf,<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configurable)>
Value Relationship Dependency,yarn.resourcemanager.max-completed-applications,yarn.resourcemanager.state-store.max-completed-applications,org.apache.hadoop.yarn.server.resourcemanager.RMAppManager,"<org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: void <init>(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,org.apache.hadoop.yarn.server.security.ApplicationACLsManager,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,yarn.nodemanager.resourcemanager.connect.max-wait.ms,yarn.nodemanager.resourcemanager.connect.retry-interval.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Value Relationship Dependency,dfs.namenode.edits.dir,dfs.namenode.edits.dir.required,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,fs.s3a.server-side-encryption-algorithm,fs.s3a.server-side-encryption-key,org.apache.hadoop.fs.s3a.S3AUtils,<org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.S3AEncryptionMethods getEncryptionAlgorithm(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,dfs.namenode.stale.datanode.interval,dfs.namenode.stale.datanode.minimum.interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
Value Relationship Dependency,fs.trash.checkpoint.interval,fs.trash.interval,org.apache.hadoop.fs.TrashPolicyDefault$Emptier,"<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void <init>(org.apache.hadoop.fs.TrashPolicyDefault,org.apache.hadoop.conf.Configuration,long)>"
Value Relationship Dependency,hadoop.security.kms.client.failover.sleep.base.millis,hadoop.security.kms.client.failover.sleep.max.millis,org.apache.hadoop.io.retry.RetryPolicies,"<org.apache.hadoop.io.retry.RetryPolicies: long calculateExponentialTime(long,int,long)>"
Value Relationship Dependency,fs.s3a.proxy.host,fs.s3a.proxy.port,org.apache.hadoop.fs.s3a.DefaultS3ClientFactory,"<org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: void initProxySupport(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>"
Value Relationship Dependency,io.bytes.per.checksum,io.file.buffer.size,org.apache.hadoop.fs.ChecksumFs,"<org.apache.hadoop.fs.ChecksumFs: int getSumBufferSize(int,int,org.apache.hadoop.fs.Path)>"
Value Relationship Dependency,dfs.replication,dfs.replication.max,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,boolean,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,dfs.namenode.maintenance.replication.min,dfs.replication,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,boolean,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,dfs.namenode.replication.min,dfs.replication,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,boolean,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,hadoop.security.dns.interface,hadoop.security.dns.nameserver,org.apache.hadoop.security.SecurityUtil,*
Value Relationship Dependency,dfs.namenode.heartbeat,dfs.namenode.stale.datanode.interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
Value Relationship Dependency,yarn.client.failover-sleep-base-ms,yarn.client.failover-sleep-max-ms,org.apache.hadoop.io.retry.RetryPolicies,"<org.apache.hadoop.io.retry.RetryPolicies: long calculateExponentialTime(long,int,long)>"
Value Relationship Dependency,hadoop.registry.zk.connection.timeout.ms,hadoop.registry.zk.session.timeout.ms,org.apache.curator.ConnectionState,<org.apache.curator.ConnectionState: void checkTimeouts()>
Value Relationship Dependency,yarn.nm.liveness-monitor.expiry-interval-ms,yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM,<org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: void <init>(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,dfs.namenode.checkpoint.check.period,dfs.namenode.checkpoint.period,org.apache.hadoop.hdfs.server.namenode.CheckpointConf,<org.apache.hadoop.hdfs.server.namenode.CheckpointConf: long getCheckPeriod()>
Value Relationship Dependency,yarn.nm.liveness-monitor.expiry-interval-ms,yarn.resourcemanager.nodemanagers.heartbeat-interval-ms,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void validateConfigs(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,file.bytes-per-checksum,file.stream-buffer-size,org.apache.hadoop.fs.ChecksumFileSystem,"<org.apache.hadoop.fs.ChecksumFileSystem: int getSumBufferSize(int,int)>"
Value Relationship Dependency,yarn.resourcemanager.delegation.token.max-lifetime,yarn.resourcemanager.delegation.token.renew-interval,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager,"<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: long renewToken(org.apache.hadoop.security.token.Token,java.lang.String)>"
Value Relationship Dependency,yarn.resourcemanager.connect.max-wait.ms,yarn.resourcemanager.connect.retry-interval.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Value Relationship Dependency,dfs.namenode.heartbeat.recheck-interval,dfs.namenode.stale.datanode.interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
Value Relationship Dependency,yarn.timeline-service.client.fd-retain-secs,yarn.timeline-service.client.internal-timers-ttl-secs,org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache,"<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache: void <init>(long,long,long,long)>"
Value Relationship Dependency,yarn.nm.liveness-monitor.expiry-interval-ms,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager,<org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,yarn.resourcemanager.nm-container-queuing.min-queue-length,yarn.resourcemanager.nm-container-queuing.queue-limit-stdev,org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.QueueLimitCalculator,<org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.QueueLimitCalculator: int getThreshold()>
Value Relationship Dependency,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask,"<org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask: void <init>(org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,minSessionTimeout,tickTime,org.apache.zookeeper.server.quorum.QuorumPeerConfig,<org.apache.zookeeper.server.quorum.QuorumPeerConfig: void parseProperties(java.util.Properties)>
Value Relationship Dependency,maxSessionTimeout,minSessionTimeout,org.apache.zookeeper.server.quorum.QuorumPeerConfig,<org.apache.zookeeper.server.quorum.QuorumPeerConfig: void parseProperties(java.util.Properties)>
Value Relationship Dependency,dfs.blockreport.initialDelay,dfs.blockreport.intervalMsec,org.apache.hadoop.hdfs.server.datanode.DNConf,<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configurable)>
Value Relationship Dependency,yarn.nodemanager.resource.memory-mb,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.nodemanager.resource.cpu-vcores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Overwrite,yarn.resourcemanager.ha.id,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.util.RMHAUtils,<org.apache.hadoop.yarn.util.RMHAUtils: java.lang.String findActiveRMHAId(org.apache.hadoop.yarn.conf.YarnConfiguration)>
Overwrite,dfs.client.failover.connection.retries,ipc.client.connect.max.retries,org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider,"<org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider: void <init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)>"
Overwrite,dfs.client.failover.connection.retries.on.timeouts,ipc.client.connect.max.retries.on.timeouts,org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider,"<org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider: void <init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)>"
Default Value Dependency,hbase.client.pause,hbase.client.pause.cqtbe,org.apache.hadoop.hbase.client.AsyncProcess,"<org.apache.hadoop.hbase.client.AsyncProcess: void <init>(org.apache.hadoop.hbase.client.ClusterConnection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory)>"
Default Value Dependency,hbase.hstore.compaction.min,hbase.hstore.compactionThreshold,org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration,"<org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.regionserver.StoreConfigInformation)>"
Default Value Dependency,hbase.hstore.compaction.max,hbase.regionserver.thread.compaction.throttle,org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration,"<org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.regionserver.StoreConfigInformation)>"
Default Value Dependency,dfs.blocksize,dfs.client.read.prefetch.size,org.apache.hadoop.hdfs.DFSClient$Conf,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,dfs.internal.nameservices,dfs.nameservices,org.apache.hadoop.hdfs.DFSUtil,<org.apache.hadoop.hdfs.DFSUtil: java.util.Collection getInternalNameServices(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,dfs.namenode.edits.dir,dfs.namenode.name.dir,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>"
Default Value Dependency,yarn.fail-fast,yarn.resourcemanager.fail-fast,org.apache.hadoop.yarn.conf.YarnConfiguration,<org.apache.hadoop.yarn.conf.YarnConfiguration: boolean shouldRMFailFast(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,dfs.secondary.namenode.keytab.file,dfs.web.authentication.kerberos.keytab,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getSpnegoKeytabKey(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Default Value Dependency,dfs.namenode.keytab.file,dfs.web.authentication.kerberos.keytab,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getSpnegoKeytabKey(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Default Value Dependency,dfs.journalnode.keytab.file,dfs.web.authentication.kerberos.keytab,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getSpnegoKeytabKey(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Default Value Dependency,mapreduce.task.profile.params,mapreduce.task.profile.reduce.params,org.apache.hadoop.mapred.MapReduceChildJVM,"<org.apache.hadoop.mapred.MapReduceChildJVM: java.util.List getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId)>"
Default Value Dependency,mapreduce.task.profile.map.params,mapreduce.task.profile.params,org.apache.hadoop.mapred.MapReduceChildJVM,"<org.apache.hadoop.mapred.MapReduceChildJVM: java.util.List getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId)>"
Default Value Dependency,hadoop.security.group.mapping.ldap.base,hadoop.security.group.mapping.ldap.userbase,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,hadoop.security.group.mapping.ldap.base,hadoop.security.group.mapping.ldap.groupbase,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,yarn.client.failover-max-attempts,yarn.client.failover-sleep-base-ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Default Value Dependency,yarn.nodemanager.resource.cpu-vcores,yarn.nodemanager.resource.pcores-vcores-multiplier,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getVCoresInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Default Value Dependency,yarn.nodemanager.bind-host,yarn.nodemanager.webapp.https.address,org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer,<org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: void serviceStart()>
Default Value Dependency,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask,"<org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask: void <init>(org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService,org.apache.hadoop.conf.Configuration)>"
Default Value Dependency,yarn.nodemanager.container-monitor.interval-ms,yarn.nodemanager.resource-monitor.interval-ms,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,yarn.resourcemanager.bind-host,yarn.resourcemanager.webapp.address,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void serviceInit(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,yarn.nodemanager.resourcemanager.connect.max-wait.ms,yarn.resourcemanager.connect.max-wait.ms,org.apache.hadoop.yarn.server.api.ServerRMProxy,"<org.apache.hadoop.yarn.server.api.ServerRMProxy: java.lang.Object createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class)>"
Default Value Dependency,yarn.nodemanager.resourcemanager.connect.retry-interval.ms,yarn.resourcemanager.connect.retry-interval.ms,org.apache.hadoop.yarn.server.api.ServerRMProxy,"<org.apache.hadoop.yarn.server.api.ServerRMProxy: java.lang.Object createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class)>"
Default Value Dependency,yarn.timeline-service.bind-host,yarn.timeline-service.webapp.https.address,org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer,<org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: void startWebApp()>
Default Value Dependency,yarn.client.failover-sleep-base-ms,yarn.resourcemanager.connect.retry-interval.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Default Value Dependency,yarn.client.failover-sleep-max-ms,yarn.resourcemanager.connect.retry-interval.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Default Value Dependency,yarn.client.failover-max-attempts,yarn.resourcemanager.connect.max-wait.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Default Value Dependency,yarn.nodemanager.container-monitor.resource-calculator.class,yarn.nodemanager.resource-calculator.class,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,<org.apache.hadoop.yarn.util.ResourceCalculatorPlugin: org.apache.hadoop.yarn.util.ResourceCalculatorPlugin getContainersMonitorPlugin(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,minSessionTimeout,tickTime,org.apache.zookeeper.server.quorum.QuorumPeerConfig,<org.apache.zookeeper.server.quorum.QuorumPeerConfig: void parseProperties(java.util.Properties)>
Default Value Dependency,maxSessionTimeout,tickTime,org.apache.zookeeper.server.quorum.QuorumPeerConfig,<org.apache.zookeeper.server.quorum.QuorumPeerConfig: void parseProperties(java.util.Properties)>
Default Value Dependency,dfs.datanode.lifeline.interval.seconds,dfs.heartbeat.interval,org.apache.hadoop.hdfs.server.datanode.DNConf,<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configurable)>
Default Value Dependency,dfs.namenode.replication.min,dfs.namenode.safemode.replication.min,org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,boolean,org.apache.hadoop.conf.Configuration)>"
Default Value Dependency,dfs.web.authentication.kerberos.keytab,nfs.keytab.file,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String getSpnegoKeytabKey(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Default Value Dependency,yarn.resourcemanager.admin.address,yarn.resourcemanager.hostname,*,*
Default Value Dependency,yarn.timeline-service.hostname,yarn.timeline-service.webapp.address,*,*
Default Value Dependency,yarn.timeline-service.address,yarn.timeline-service.hostname,*,*
Default Value Dependency,yarn.timeline-service.hostname,yarn.timeline-service.webapp.https.address,*,*
Default Value Dependency,yarn.resourcemanager.max-completed-applications,yarn.resourcemanager.state-store.max-completed-applications,*,*
Default Value Dependency,yarn.nodemanager.hostname,yarn.nodemanager.localizer.address,*,*
Default Value Dependency,yarn.nodemanager.hostname,yarn.nodemanager.webapp.address,*,*
Default Value Dependency,yarn.nodemanager.address,yarn.nodemanager.hostname,*,*
Default Value Dependency,yarn.resourcemanager.address,yarn.resourcemanager.hostname,*,*
Default Value Dependency,yarn.nodemanager.collector-service.address,yarn.nodemanager.hostname,*,*
Default Value Dependency,yarn.resourcemanager.hostname,yarn.resourcemanager.webapp.address,*,*
Default Value Dependency,yarn.resourcemanager.hostname,yarn.resourcemanager.resource-tracker.address,*,*
Default Value Dependency,yarn.resourcemanager.hostname,yarn.resourcemanager.scheduler.address,*,*
Default Value Dependency,yarn.resourcemanager.hostname,yarn.resourcemanager.webapp.https.address,*,*
Default Value Dependency,dfs.namenode.kerberos.internal.spnego.principal,dfs.web.authentication.kerberos.principal,*,*
Default Value Dependency,dfs.namenode.checkpoint.dir,dfs.namenode.checkpoint.edits.dir,*,*
Default Value Dependency,dfs.journalnode.kerberos.internal.spnego.principal,dfs.web.authentication.kerberos.keytab,*,*
Default Value Dependency,alluxio.conf.dir,alluxio.underfs.hdfs.configuration,*,*
Default Value Dependency,alluxio.master.mount.table.root.ufs,alluxio.underfs.address,*,*
Default Value Dependency,alluxio.conf.dir,alluxio.site.conf.dir,*,*
Default Value Dependency,alluxio.conf.dir,alluxio.home,*,*
Default Value Dependency,alluxio.conf.dir,alluxio.metrics.conf.file,*,*
Default Value Dependency,alluxio.home,alluxio.work.dir,*,*
Default Value Dependency,alluxio.logserver.logs.dir,alluxio.work.dir,*,*
Default Value Dependency,alluxio.master.journal.folder,alluxio.work.dir,*,*
Default Value Dependency,alluxio.logs.dir,alluxio.work.dir,*,*
Default Value Dependency,alluxio.underfs.address,alluxio.work.dir,*,*
Default Value Dependency,alluxio.extensions.dir,alluxio.home,*,*
Default Value Dependency,alluxio.logs.dir,alluxio.master.lineage.recompute.log.path,*,*
Default Value Dependency,hbase.dynamic.jars.dir,hbase.rootdir,*,*
Default Value Dependency,hbase.rootdir,hbase.tmp.dir,*,*
Default Value Dependency,hbase.tmp.dir,hbase.zookeeper.property.dataDir,*,*
Default Value Dependency,hbase.local.dir,hbase.tmp.dir,*,*
Default Value Dependency,spark.pyspark.driver.python,spark.pyspark.python,*,*
Default Value Dependency,spark.network.timeout,spark.rpc.asktimeout,*,*
Default Value Dependency,spark.network.timeout,spark.rpc.lookuptimeout,*,*
Default Value Dependency,spark.executor.memory,spark.executor.memoryoverhead,*,*
Default Value Dependency,spark.driver.bindaddress,spark.driver.host,*,*
Default Value Dependency,spark.driver.memory,spark.driver.memoryoverhead,*,*
Behavior Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxmaps,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxreduces,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxmaps,mapreduce.job.ubertask.maxreduces,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.enable,mapreduce.job.ubertask.maxbytes,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxbytes,mapreduce.job.ubertask.maxmaps,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxbytes,mapreduce.job.ubertask.maxreduces,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.enable,mapreduce.map.memory.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxmaps,mapreduce.map.memory.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxreduces,mapreduce.map.memory.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxbytes,mapreduce.map.memory.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.enable,mapreduce.map.cpu.vcores,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxmaps,mapreduce.map.cpu.vcores,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxreduces,mapreduce.map.cpu.vcores,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.job.ubertask.maxbytes,mapreduce.map.cpu.vcores,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.map.cpu.vcores,mapreduce.map.memory.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Behavior Dependency,mapreduce.map.sort.spill.percent,mapreduce.task.io.sort.mb,org.apache.hadoop.mapred.MapTask$MapOutputBuffer,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void init(org.apache.hadoop.mapred.MapOutputCollector$Context)>
Behavior Dependency,mapreduce.map.skip.maxrecords,mapreduce.map.skip.proc-count.auto-incr,org.apache.hadoop.mapred.lib.MultithreadedMapRunner,<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void configure(org.apache.hadoop.mapred.JobConf)>
Behavior Dependency,mapreduce.reduce.skip.maxgroups,mapreduce.reduce.skip.proc-count.auto-incr,org.apache.hadoop.mapred.ReduceTask,"<org.apache.hadoop.mapred.ReduceTask: void runOldReducer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapred.RawKeyValueIterator,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class)>"
Behavior Dependency,mapreduce.job.cache.limit.max-resources,mapreduce.job.cache.limit.max-resources-mb,org.apache.hadoop.mapreduce.JobResourceUploader$LimitChecker,<org.apache.hadoop.mapreduce.JobResourceUploader$LimitChecker: boolean hasLimits()>
Behavior Dependency,yarn.app.mapreduce.am.container.log.backups,yarn.app.mapreduce.am.container.log.limit.kb,org.apache.hadoop.mapreduce.v2.util.MRApps,"<org.apache.hadoop.mapreduce.v2.util.MRApps: void addLog4jSystemProperties(org.apache.hadoop.mapred.Task,java.util.List,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,yarn.app.mapreduce.shuffle.log.backups,yarn.app.mapreduce.shuffle.log.limit.kb,org.apache.hadoop.mapreduce.v2.util.MRApps,"<org.apache.hadoop.mapreduce.v2.util.MRApps: void addLog4jSystemProperties(org.apache.hadoop.mapred.Task,java.util.List,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,mapreduce.job.reduces,mapreduce.task.io.sort.mb,org.apache.hadoop.mapred.MapTask$MapOutputBuffer,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void sortAndSpill()>
Behavior Dependency,hbase.zookeeper.peerport,hbase.zookeeper.quorum,org.apache.hadoop.hbase.zookeeper.ZKConfig,<org.apache.hadoop.hbase.zookeeper.ZKConfig: java.util.Properties makeZKPropsFromHbaseConfig(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,hbase.mob.cache.evict.remain.ratio,hbase.mob.file.cache.size,org.apache.hadoop.hbase.mob.MobFileCache,<org.apache.hadoop.hbase.mob.MobFileCache: void evict()>
Behavior Dependency,hbase.hstore.blockingStoreFiles,hbase.hstore.compaction.min,org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager,<org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager: double getCompactionPressure()>
Behavior Dependency,hbase.client.scanner.timeout.period,hbase.rpc.timeout,org.apache.hadoop.hbase.regionserver.RSRpcServices,"<org.apache.hadoop.hbase.regionserver.RSRpcServices: long getTimeLimit(org.apache.hadoop.hbase.ipc.HBaseRpcController,boolean)>"
Behavior Dependency,hbase.mob.compaction.batch.size,hbase.mob.delfile.max.count,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor,"<org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor: java.util.List compactDelFiles(org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactionRequest,java.util.List)>"
Behavior Dependency,hbase.coprocessor.enabled,hbase.coprocessor.user.enabled,org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost,"<org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost: void <init>(org.apache.hadoop.hbase.regionserver.RegionServerServices,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,hbase.hregion.memstore.block.multiplier,hbase.hregion.memstore.flush.size,org.apache.hadoop.hbase.regionserver.HRegion,<org.apache.hadoop.hbase.regionserver.HRegion: void setHTableSpecificConf()>
Behavior Dependency,hbase.hregion.majorcompaction,hbase.hregion.majorcompaction.jitter,org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy,<org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy: long getNextMajorCompactTime(java.util.Collection)>
Behavior Dependency,hbase.regionserver.global.memstore.size,hfile.block.cache.size,org.apache.hadoop.hbase.regionserver.HeapMemoryManager,<org.apache.hadoop.hbase.regionserver.HeapMemoryManager: boolean doInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,hbase.offpeak.end.hour,hbase.offpeak.start.hour,org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours,"<org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours: org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours getInstance(int,int)>"
Behavior Dependency,hbase.ipc.server.callqueue.read.ratio,hbase.ipc.server.callqueue.scan.ratio,org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor,"<org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor: void <init>(java.lang.String,int,int,org.apache.hadoop.hbase.ipc.PriorityFunction,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.Abortable)>"
Behavior Dependency,hbase.hstore.compaction.min,hbase.hstore.compaction.ratio.offpeak,org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy,"<org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy: boolean filesInRatio(java.util.List,double)>"
Behavior Dependency,hbase.zookeeper.property.clientPort,hbase.zookeeper.quorum,org.apache.hadoop.hbase.zookeeper.ZKConfig,"<org.apache.hadoop.hbase.zookeeper.ZKConfig: java.lang.String buildZKQuorumServerString(java.lang.String[],java.lang.String)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.search.filter.group,hadoop.security.group.mapping.ldap.search.filter.user,org.apache.hadoop.security.LdapGroupsMapping,<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,io.compression.codec.bzip2.library,io.native.lib.available,org.apache.hadoop.io.compress.bzip2.Bzip2Factory,<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,io.mapfile.bloom.error.rate,io.mapfile.bloom.size,org.apache.hadoop.io.BloomMapFile$Writer,<org.apache.hadoop.io.BloomMapFile$Writer: void initBloomFilter(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,hadoop.security.group.mapping.ldap.search.attr.member,hadoop.security.group.mapping.ldap.search.filter.group,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: java.util.List lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.groupbase,hadoop.security.group.mapping.ldap.search.filter.group,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: java.util.List lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.posix.attr.gid.name,hadoop.security.group.mapping.ldap.posix.attr.uid.name,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: javax.naming.NamingEnumeration lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.posix.attr.gid.name,hadoop.security.group.mapping.ldap.search.filter.group,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: javax.naming.NamingEnumeration lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.posix.attr.gid.name,hadoop.security.group.mapping.ldap.search.attr.member,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: javax.naming.NamingEnumeration lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.groupbase,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: java.util.List lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)>"
Behavior Dependency,hadoop.security.group.mapping.ldap.search.filter.group,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,org.apache.hadoop.security.LdapGroupsMapping,"<org.apache.hadoop.security.LdapGroupsMapping: java.util.List lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)>"
Behavior Dependency,fs.ftp.host,fs.ftp.host.port,org.apache.hadoop.fs.ftp.FTPFileSystem,<org.apache.hadoop.fs.ftp.FTPFileSystem: org.apache.commons.net.ftp.FTPClient connect()>
Behavior Dependency,yarn.resourcemanager.ha.automatic-failover.embedded,yarn.resourcemanager.ha.automatic-failover.enabled,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: void verifyLeaderElection(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.timeline-service.enabled,yarn.timeline-service.version,org.apache.hadoop.yarn.conf.YarnConfiguration,<org.apache.hadoop.yarn.conf.YarnConfiguration: boolean timelineServiceV2Enabled(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.resourcemanager.ha.id,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.conf.HAUtil,<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getRMHAId(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.remote-app-log-dir,yarn.nodemanager.remote-app-log-dir-suffix,org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory,<org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory: void <init>(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.linux-container-executor.cgroups.mount,yarn.nodemanager.linux-container-executor.cgroups.mount-path,org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler,"<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin)>"
Behavior Dependency,yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler,"<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void setupLimits(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource)>"
Behavior Dependency,yarn.nodemanager.resource.cpu-vcores,yarn.nodemanager.resource.percentage-physical-cpu-limit,org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor,"<org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: java.lang.String[] getRunCommandForWindows(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Resource)>"
Behavior Dependency,yarn.nodemanager.resource.memory-mb,yarn.nodemanager.vmem-pmem-ratio,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.pmem-check-enabled,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.system-metrics-publisher.enabled,yarn.timeline-service.enabled,org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl,"<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void <init>(org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor,org.apache.hadoop.yarn.server.nodemanager.DeletionService,org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater,org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics,org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService)>"
Behavior Dependency,yarn.nodemanager.process-kill-wait.ms,yarn.nodemanager.sleep-delay-before-sigkill.ms,org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.address,yarn.nodemanager.bind-host,org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void serviceStart()>
Behavior Dependency,yarn.nodemanager.recovery.enabled,yarn.nodemanager.recovery.supervised,org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl,<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: boolean isNMUnderSupervisionWithRecoveryEnabled()>
Behavior Dependency,yarn.acl.enable,yarn.acl.reservation-enable,org.apache.hadoop.yarn.server.resourcemanager.security.ReservationsACLsManager,"<org.apache.hadoop.yarn.server.resourcemanager.security.ReservationsACLsManager: void <init>(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,yarn.resourcemanager.ha.automatic-failover.enabled,yarn.resourcemanager.ha.enabled,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore,<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void startInternal()>
Behavior Dependency,yarn.resourcemanager.webapp.cross-origin.enabled,yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.zk-state-store.root-node.acl,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.util.curator.ZKCuratorManager createAndStartZKManager(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.distributed-scheduling.enabled,yarn.resourcemanager.opportunistic-container-allocation.enabled,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService createApplicationMasterService()>
Behavior Dependency,yarn.node-labels.configuration-type,yarn.node-labels.enabled,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater createRMDelegatedNodeLabelsUpdater()>
Behavior Dependency,yarn.resourcemanager.cluster-id,yarn.resourcemanager.ha.automatic-failover.zk-base-path,org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService,<org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService: void serviceInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.container-monitor.enabled,yarn.nodemanager.container-monitor.interval-ms,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl,<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,yarn.nodemanager.resource.pcores-vcores-multiplier,yarn.nodemanager.resource.percentage-physical-cpu-limit,org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils,"<org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils: int getVCoresInternal(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.ha.id,org.apache.hadoop.yarn.webapp.util.WebAppUtils,"<org.apache.hadoop.yarn.webapp.util.WebAppUtils: java.lang.String getRMWebAppURLWithoutScheme(org.apache.hadoop.conf.Configuration,boolean)>"
Behavior Dependency,yarn.client.failover-sleep-base-ms,yarn.resourcemanager.connect.max-wait.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Behavior Dependency,yarn.resourcemanager.address,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.conf.HAUtil,"<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String addSuffix(java.lang.String,java.lang.String)>"
Behavior Dependency,yarn.client.failover-sleep-base-ms,yarn.nodemanager.resourcemanager.connect.max-wait.ms,org.apache.hadoop.yarn.client.RMProxy,"<org.apache.hadoop.yarn.client.RMProxy: org.apache.hadoop.io.retry.RetryPolicy createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean)>"
Behavior Dependency,yarn.resourcemanager.ha.rm-ids,yarn.resourcemanager.webapp.https.address,org.apache.hadoop.yarn.conf.HAUtil,"<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String addSuffix(java.lang.String,java.lang.String)>"
Behavior Dependency,yarn.resourcemanager.ha.rm-ids,yarn.resourcemanager.webapp.address,org.apache.hadoop.yarn.conf.HAUtil,"<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String addSuffix(java.lang.String,java.lang.String)>"
Behavior Dependency,yarn.resourcemanager.webapp.spnego-keytab-file,yarn.resourcemanager.webapp.spnego-principal,org.apache.hadoop.yarn.webapp.WebApps$Builder,<org.apache.hadoop.yarn.webapp.WebApps$Builder: org.apache.hadoop.yarn.webapp.WebApp build(org.apache.hadoop.yarn.webapp.WebApp)>
Behavior Dependency,yarn.resourcemanager.address,yarn.resourcemanager.principal,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: java.lang.String replacePattern(java.lang.String[],java.lang.String)>"
Behavior Dependency,hadoop.http.filter.initializers,hadoop.security.authentication,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,hadoop.http.staticuser.user,hadoop.security.authentication,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices: javax.ws.rs.core.Response updateAppState(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppState,javax.servlet.http.HttpServletRequest,java.lang.String)>"
Behavior Dependency,hadoop.http.authentication.type,hadoop.security.authentication,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: void render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)>
Behavior Dependency,dfs.heartbeat.interval,dfs.namenode.replication.interval,org.apache.hadoop.hdfs.server.mover.Mover,"<org.apache.hadoop.hdfs.server.mover.Mover: int run(java.util.Map,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,hadoop.common.configuration.version,hadoop.hdfs.configuration.version,org.apache.hadoop.hdfs.server.datanode.DataNode,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Behavior Dependency,dfs.namenode.handler.count,dfs.namenode.lifeline.handler.ratio,org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>"
Behavior Dependency,dfs.namenode.checkpoint.txns,dfs.namenode.edit.log.autoroll.multiplier.threshold,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Behavior Dependency,dfs.namenode.avoid.write.stale.datanode,dfs.namenode.stale.datanode.interval,org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager,"<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,dfs.heartbeat.interval,dfs.namenode.heartbeat.recheck-interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,dfs.heartbeat.interval,dfs.namenode.stale.datanode.minimum.interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
Behavior Dependency,dfs.namenode.avoid.write.stale.datanode,dfs.namenode.write.stale.datanode.ratio,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: boolean shouldAvoidStaleDataNodesForWrite()>
Behavior Dependency,rpc.metrics.percentiles.intervals,rpc.metrics.quantile.enable,org.apache.hadoop.ipc.metrics.RpcMetrics,"<org.apache.hadoop.ipc.metrics.RpcMetrics: void <init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,dfs.namenode.num.checkpoints.retained,dfs.namenode.num.extra.edits.retained,org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>
Behavior Dependency,dfs.ha.automatic-failover.enabled,dfs.ha.zkfc.port,org.apache.hadoop.hdfs.tools.NNHAServiceTarget,"<org.apache.hadoop.hdfs.tools.NNHAServiceTarget: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>"
Behavior Dependency,dfs.client-write-packet-size,dfs.client.socket-timeout,org.apache.hadoop.hdfs.DFSClient,<org.apache.hadoop.hdfs.DFSClient: int getDatanodeReadTimeout(int)>
Behavior Dependency,dfs.namenode.replication.considerLoad,dfs.namenode.replication.considerLoad.factor,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: boolean isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)>"
Behavior Dependency,dfs.webhdfs.socket.connect-timeout,dfs.webhdfs.socket.read-timeout,org.apache.hadoop.hdfs.web.URLConnectionFactory,"<org.apache.hadoop.hdfs.web.URLConnectionFactory: org.apache.hadoop.security.authentication.client.ConnectionConfigurator getSSLConnectionConfiguration(int,int,org.apache.hadoop.conf.Configuration)>"
Behavior Dependency,dfs.namenode.rpc-address,dfs.nameservices,org.apache.hadoop.hdfs.DFSUtil,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>"
Behavior Dependency,dfs.namenode.avoid.read.stale.datanode,dfs.namenode.stale.datanode.interval,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: boolean isInactive(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
Behavior Dependency,dfs.namenode.checkpoint.period,dfs.namenode.rpc-address,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>
Behavior Dependency,dfs.journalnode.https-address,dfs.journalnode.https-bind-host,org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer,<org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer: void start()>
Behavior Dependency,dfs.namenode.https-address,dfs.namenode.https-bind-host,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void start()>
Behavior Dependency,dfs.namenode.https-address,dfs.namenode.rpc-address,org.apache.hadoop.hdfs.DFSUtilClient,"<org.apache.hadoop.hdfs.DFSUtilClient: java.lang.String addSuffix(java.lang.String,java.lang.String)>"
Behavior Dependency,dfs.namenode.http-address,dfs.namenode.rpc-address,org.apache.hadoop.hdfs.DFSUtilClient,"<org.apache.hadoop.hdfs.DFSUtilClient: java.lang.String addSuffix(java.lang.String,java.lang.String)>"
Behavior Dependency,spark.storage.memoryfraction,spark.storage.safetyfraction,org.apache.spark.memory.StaticMemoryManager,*